---
title: "Gemini æŒ‡å—"
description: "åŒå­æ˜Ÿæ¼«æ¸¸æŒ‡å—ï¼šå…³äºæœ¬ç«™çš„ Gemini è°ƒç”¨ç»†èŠ‚ï¼Œåœ¨æ­¤æ±‡æ€»ã€‚"
icon: "google"
---


## Gemini è°ƒç”¨æ–¹å¼

å¯¹äº Gemini ç³»åˆ—ï¼Œæˆ‘ä»¬æä¾›åŸç”Ÿè°ƒç”¨å’Œ Openai å…¼å®¹è¿™ 2 ç§è°ƒç”¨æ–¹å¼ã€‚\
ä½¿ç”¨å‰è¿è¡Œ `pip install google-genai` æˆ– `pip install -U google-genai`ï¼Œå®‰è£…ï¼ˆæ›´æ–°ï¼‰åŸç”Ÿä¾èµ–ã€‚

1ï¸âƒ£ å¯¹äºåŸç”Ÿè°ƒç”¨ï¼Œæˆ‘ä»¬çš„ Gemini è°ƒç”¨æ”¯æŒ AI Studio å’Œ VertexAI è‡ªåŠ¨è·¯ç”±ã€‚è½¬å‘æ–¹æ³•ä¸»è¦æ˜¯åœ¨å†…éƒ¨ä¼ å…¥ AiHubMix å¯†é’¥å’Œè¯·æ±‚é“¾æ¥ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªé“¾æ¥å’Œå¸¸è§„çš„ `base_url` å†™æ³•ä¸åŒï¼Œè¯·å‚è€ƒç¤ºä¾‹ï¼š

```py
client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"},
)
```

2ï¸âƒ£ å¯¹äº Openai å…¼å®¹æ ¼å¼ï¼Œåˆ™ç»´æŒé€šç”¨çš„ `v1` ç«¯ç‚¹ï¼š

```py
client = OpenAI(
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)
```

3ï¸âƒ£ å¯¹äº 2.5 ç³»åˆ—ï¼Œå¦‚æœä½ éœ€è¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ 2 ç§æ–¹å¼ï¼š

1. åŸç”Ÿè°ƒç”¨ï¼šä¼ å…¥ `include_thoughts=True`
2. OpenAI å…¼å®¹æ–¹å¼ï¼šä¼ å…¥ `reasoning_effort`

ç›¸å…³çš„è¯¦ç»†è°ƒç”¨å¯ä»¥å‚è€ƒä¸‹æ–‡çš„ä»£ç ç¤ºä¾‹ã€‚

### Gemini 2.5 ç³»åˆ—çš„ã€Œæ¨ç†ã€è¯´æ˜

1. 2.5 ç³»åˆ—éƒ½æ˜¯æ¨ç†æ¨¡å‹ã€‚
2. 2.5 flash æ˜¯æ··åˆæ¨¡å‹ï¼Œç±»ä¼¼ claude sonnet 3.7ï¼Œå¯ä»¥é€šè¿‡ç”¨ `thinking_budget` æ§åˆ¶æ¨ç†é¢„ç®—æ¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚
3. 2.5 pro æ˜¯çº¯ç²¹çš„æ¨ç†æ¨¡å‹ï¼Œå› æ­¤ä¸èƒ½å…³é—­ thinkingã€ä¹Ÿä¸æ˜¾å¼ä¼ é€’æ¨ç†é¢„ç®—ã€‚

**Python è°ƒç”¨å‚è€ƒå¦‚ä¸‹ï¼š**

<CodeGroup>

```py æ™®é€šéæµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å¯¹äºæ™®é€šè‚¡ç¥¨æŠ•èµ„è€…ï¼šåˆ†æè´¢æŠ¥æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]

    print(client.models.generate_content(
        model=model,
        contents=contents,
    ))

if __name__ == "__main__":
    generate()
```


```py 2.0 ç³»åˆ—-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å¯¹äºæ™®é€šè‚¡ç¥¨æŠ•èµ„è€…ï¼šåˆ†æè´¢æŠ¥æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py 2.5 Flash-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-flash-preview-04-17" #gemini-2.5-pro-preview-03-25ã€gemini-2.5-flash-preview-04-17
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å¯¹äºæ™®é€šè‚¡ç¥¨æŠ•èµ„è€…ï¼šåˆ†æè´¢æŠ¥æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        thinking_config = types.ThinkingConfig(
            thinking_budget=2048, #èŒƒå›´ 0-16384ã€‚é»˜è®¤ 1024ï¼Œæœ€ä½³è¾¹é™…æ•ˆæœ 16000
        ),
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py 2.5 Pro-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-03-25"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""æ€ä¹ˆçŸ¥é“æˆ‘ä¸æ˜¯åœ¨æµªè´¹æ—¶é—´"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py æ˜¾ç¤ºæ¨ç†å†…å®¹
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-05-06"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""é‡‘èé¢†åŸŸçš„ã€Œ72 æ³•åˆ™ã€æ˜¯å¦‚ä½•æ¨å¯¼çš„ï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
        thinking_config=types.ThinkingConfig(
            include_thoughts=True  # ğŸ§  å¯ç”¨æ€è€ƒè¿‡ç¨‹è¾“å‡º
        ),
    )

    # ç”¨äºå­˜å‚¨æœ€åä¸€ä¸ª chunk çš„ usage_metadata
    final_usage_metadata = None
    
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹éƒ¨åˆ†
        if chunk.candidates and len(chunk.candidates) > 0:
            for part in chunk.candidates[0].content.parts:
                if part.text:
                    if part.thought:
                        # æ€è€ƒè¿‡ç¨‹å†…å®¹
                        print(part.text, end="")
                    else:
                        # æœ€ç»ˆç­”æ¡ˆå†…å®¹
                        print(part.text, end="")
        
        # ä¿å­˜æœ€æ–°çš„ usage_metadataï¼Œåªæœ‰æœ€åä¸€ä¸ª chunk ä¼šåŒ…å«å®Œæ•´ä¿¡æ¯
        if chunk.usage_metadata:
            final_usage_metadata = chunk.usage_metadata
    
    # åœ¨æ‰€æœ‰ chunk å¤„ç†å®Œåï¼Œæ‰“å°å®Œæ•´çš„ token ä½¿ç”¨æƒ…å†µ
    if final_usage_metadata:
        print(f"\n\nğŸ“Š Token ä½¿ç”¨æƒ…å†µ:")
        print(f"æ€è€ƒ tokens: {getattr(final_usage_metadata, 'thoughts_token_count', 'ä¸å¯ç”¨')}")
        print(f"è¾“å‡º tokens: {getattr(final_usage_metadata, 'candidates_token_count', 'ä¸å¯ç”¨')}")
        print(f"æ€»è®¡: {final_usage_metadata}")

if __name__ == "__main__":
    generate()
```

</CodeGroup>

## Gemini 2.5 Flash æ”¯æŒ

Openai å…¼å®¹æ–¹å¼è°ƒç”¨å‚è€ƒå¦‚ä¸‹ï¼š

<CodeGroup>

```py Python ç”¨äºå¿«é€Ÿä»»åŠ¡æ—¶ï¼Œå…³é—­æ€è€ƒ
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17-nothink",
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ]
)

print(completion.choices[0].message.content)
```


```py Python æ§åˆ¶é¢„ç®—
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17",
    reasoning_effort="low", # å¯é€‰ "low", "medium" å’Œ "high", åˆ†åˆ«å¯¹åº” 1024, 8192 å’Œ 16384 æ¨ç†é¢„ç®—
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ]
)

print(completion.choices[0].message.content)
```


```shell Curl-åŸºç¡€è°ƒç”¨
curl -X POST https://aihubmix.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-***" \
  -d '{
    "model": "gemini-2.5-flash-preview-04-17-nothink",
    "messages": [
      {
        "role": "user",
        "content": "Explain the Occam'\''s Razor concept and provide an everyday example of it."
      }
    ]
  }'
```


```shell Curl-Thinking æ˜¾ç¤º
curl https://aihubmix.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer sk-***" \
-d '{
  "model": "gemini-2.5-pro-preview-05-06",
  "messages": [
    {
      "role": "user",
      "content": "Explain the Occam'\''s Razor concept and provide an everyday example of it."
    }
  ],
  "reasoning_effort": "low"
}'
```

</CodeGroup>

<Tip>
  1. ç”¨äºå¤æ‚ä»»åŠ¡æ—¶ï¼Œåªéœ€è¦å°†æ¨¡å‹ id è®¾ç½®ä¸ºé»˜è®¤å¼€å¯æ€è€ƒçš„ `gemini-2.5-flash-preview-04-17` å³å¯ã€‚
  2. Gemini 2.5 Flash é€šè¿‡ `budget`ï¼ˆæ€è€ƒé¢„ç®—ï¼‰æ¥æ§åˆ¶æ€è€ƒçš„æ·±åº¦ï¼ŒèŒƒå›´ 0-16Kï¼Œç›®å‰è½¬å‘é‡‡ç”¨çš„æ˜¯é»˜è®¤é¢„ç®— 1024ï¼Œæœ€ä½³è¾¹é™…æ•ˆæœä¸º 16Kã€‚
</Tip>

## å¤šåª’ä½“æ–‡ä»¶

Aihubmix ç›®å‰åªæ”¯æŒ**å°äº 20MB** çš„å¤šåª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ï¼Œç”¨ `inline_data` ä¸Šä¼ ã€‚\
å¤§äº 20M çš„å¤šåª’ä½“éœ€è¦ç”¨ File APIï¼ˆå°šæœªæ”¯æŒï¼‰ï¼Œå¾…å®Œå–„çŠ¶æ€è·Ÿè¸ªï¼Œè¿”å› `upload_url`ã€‚

<Tip>
  ä½ å¯ä»¥å¢åŠ  `EDIARESOLUTION_MEDIUM` å‚æ•°æ¥çº¦æŸå›¾ç‰‡çš„ç²¾åº¦ï¼Œä»è€Œå¤§å¹…èŠ‚çœè¾“å…¥çš„è´¹ç”¨ä»¥åŠå‡å°‘å¤§å›¾æŠ¥é”™çš„å¯èƒ½æ€§ã€‚
</Tip>

<CodeGroup>

```py å›¾ç‰‡
from google import genai
from google.genai import types

# è¯»å–æ–‡ä»¶ä¸ºäºŒè¿›åˆ¶æ•°æ®
file_path = "yourpath/file.jpeg"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="image/jpeg"
                )
            ),
            types.Part(
                text="Describe the image."
            )
        ]
    ),
    config=types.GenerateContentConfig(
        system_instruction="You are a helpful assistant that can describe images.",
        max_output_tokens=768,
        temperature=0.1,
        thinking_config=types.ThinkingConfig(
            thinking_budget=0, include_thoughts=False
        ),
        media_resolution=types.MediaResolution.MEDIA_RESOLUTION_MEDIUM # 256 tokens
    )
)

print(response.text)
print(response.usage_metadata) # è¾“å‡º token èŠ±è´¹ç»†èŠ‚
```


```py éŸ³é¢‘
from google import genai
from google.genai import types

# è¯»å–æ–‡ä»¶ä¸ºäºŒè¿›åˆ¶æ•°æ®
file_path = "yourpath/file.m4a"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="audio/m4a"
                )
            ),
            types.Part(
                text="Transcribe the audio to text."
            )
        ]
    )
)

print(response.text)
```


```py è§†é¢‘
from google import genai
from google.genai import types

# è¯»å–æ–‡ä»¶ä¸ºäºŒè¿›åˆ¶æ•°æ®
file_path = "yourpath/file.mp4"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="video/mp4"
                )
            ),
            types.Part(
                text="Summarize this video. Then create a quiz with an answer key based on the information in this video."
            )
        ]
    )
)

print(response.text)
```


```py Youtube é“¾æ¥
from google import genai
from google.genai import types

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(
                    file_uri="https://www.youtube.com/watch?v=OoU7PwNyYUw"
                )
            ),
            types.Part(
                text="Please summarize the video in 3 sentences."
            )
        ]
    )
)

print(response.text)
```

</CodeGroup>

## Code Execution

è‡ªåŠ¨ä»£ç è§£æå™¨ç”¨ä¾‹å‚è€ƒï¼š

```py Python
from google import genai
from google.genai import types

# è¯»å–æ–‡ä»¶ä¸ºäºŒè¿›åˆ¶æ•°æ®
file_path = "yourpath/file.csv"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="text/csv"
                )
            ),
            types.Part(
                text="Please analyze this CSV and summarize the key statistics. Use code execution if needed."
            )
        ]
    ),
    config=types.GenerateContentConfig(
        tools=[types.Tool(
            code_execution=types.ToolCodeExecution
        )]
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if getattr(part, "executable_code", None) is not None:
        print("Generated code:\n", part.executable_code.code)
    if getattr(part, "code_execution_result", None) is not None:
        print("Execution result:\n", part.code_execution_result.output)
```

## ä¸Šä¸‹æ–‡ç¼“å­˜

Gemini åœ¨åŸç”Ÿ API ä¸‹é»˜è®¤å¯ç”¨äº†**éšå¼ä¸Šä¸‹æ–‡ç¼“å­˜**ï¼Œæ— éœ€å¼€å‘è€…æ‰‹åŠ¨æ“ä½œã€‚æ¯ä¸€æ¬¡ `generate_content` è¯·æ±‚ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¸ºè¾“å…¥å†…å®¹å»ºç«‹ç¼“å­˜ã€‚å½“åç»­è¯·æ±‚ä¸æ­¤å‰å†…å®¹å®Œå…¨ä¸€è‡´æ—¶ï¼Œå°†ç›´æ¥å‘½ä¸­ç¼“å­˜ï¼Œè¿”å›ä¸Šä¸€æ¬¡çš„æ¨ç†ç»“æœï¼Œå¤§å¹…æå‡å“åº”é€Ÿåº¦å¹¶æœ‰æœºä¼šèŠ‚çœ token æ¶ˆè€—ã€‚

- **ç¼“å­˜è‡ªåŠ¨ç”Ÿæ•ˆï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®ã€‚**
- ç¼“å­˜ä»…åœ¨å†…å®¹ã€æ¨¡å‹ã€å‚æ•°å®Œå…¨ä¸€è‡´æ—¶ç”Ÿæ•ˆï¼›ä»»ä½•å­—æ®µä¸åŒéƒ½ä¼šè§†ä¸ºæ–°è¯·æ±‚ï¼Œä¸å‘½ä¸­ç¼“å­˜ã€‚
- ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆTTLï¼‰ç”±å¼€å‘è€…è®¾å®šï¼Œä¹Ÿå¯ä»¥ä¸è®¾ç½®ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º 1 å°æ—¶ã€‚æ— æœ€å°æˆ–æœ€å¤§æ—¶é•¿é™åˆ¶ï¼Œè´¹ç”¨å–å†³äºç¼“å­˜ token æ•°ä¸ç¼“å­˜æ—¶é—´ã€‚
  - è™½ç„¶ Google å®˜æ–¹å¯¹ TTL ä¸è®¾ä¸Šä¸‹é™ï¼Œä½†ç”±äºæˆ‘ä»¬ä½œä¸ºè½¬å‘å¹³å°ï¼Œ**ä»…æ”¯æŒæœ‰é™çš„ TTL é…ç½®èŒƒå›´ï¼Œä¸ä¿è¯æ°¸ä¹…æœ‰æ•ˆ**ã€‚

### æ³¨æ„äº‹é¡¹

- **æ— æˆæœ¬èŠ‚çœä¿è¯**ï¼šç¼“å­˜ token çš„è®¡è´¹ä¸ºè¾“å…¥åŸä»·çš„ 25%ï¼Œç†è®ºä¸Šè¾“å…¥éƒ¨åˆ†å¯æœ€å¤šèŠ‚çœ 75% æˆæœ¬ï¼Œ[**ä½† Google å®˜æ–¹å¹¶æœªæ‰¿è¯ºå¿…ç„¶èŠ‚çœ**](https://ai.google.dev/gemini-api/docs/caching?lang=python)ï¼Œå®é™…è´¦å•è¿˜éœ€ç»“åˆç¼“å­˜å‘½ä¸­ç‡ã€token ç±»å‹ä¸å­˜å‚¨æ—¶é•¿å…±åŒè¯„ä¼°ã€‚
- **ç¼“å­˜å‘½ä¸­æ¡ä»¶**ï¼šå»ºè®®å°†é‡å¤çš„ä¸Šä¸‹æ–‡æ”¾åœ¨è¯·æ±‚å‰éƒ¨ï¼Œå°†æ˜“å˜å†…å®¹ï¼ˆå¦‚ç”¨æˆ·è¾“å…¥ï¼‰ç½®äºåéƒ¨ï¼Œä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚
- **ç¼“å­˜å‘½ä¸­åé¦ˆ**ï¼šå¦‚æœå“åº”ç»“æœå‘½ä¸­ç¼“å­˜ï¼Œåœ¨ `response.usage_metadata` ä¸­ä¼šåŒ…å« `cache_tokens_details` å­—æ®µï¼Œå¹¶æœ‰ `cached_content_token_count`ï¼Œå¼€å‘è€…å¯ä»¥æ®æ­¤åˆ¤æ–­æœ¬æ¬¡è¯·æ±‚æ˜¯å¦å‘½ä¸­ç¼“å­˜ã€‚\
  ç¤ºä¾‹å“åº”å­—æ®µï¼ˆå‘½ä¸­ç¼“å­˜æ—¶ï¼‰ï¼š

  ```
  cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=2003)]
  cached_content_token_count=2003
  ```

**ä»£ç ç¤ºä¾‹ï¼š**

```python
from google import genai

client = genai.Client(
    http_options={"base_url": "https://aihubmix.com/gemini"},
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
)

prompt = """
ã€Šä¸‰å›½æ¼”ä¹‰ã€‹è¯æ›°ï¼šæ‰“å¼€å­—å…¸    æ»šæ»šé•¿æ±Ÿä¸œé€æ°´ï¼ŒæµªèŠ±æ·˜å°½è‹±é›„ã€‚æ˜¯éæˆè´¥è½¬å¤´ç©ºï¼šé’å±±ä¾æ—§åœ¨ï¼Œå‡ åº¦å¤•é˜³çº¢ã€‚ç™½å‘æ¸”æ¨µæ±Ÿæ¸šä¸Šï¼Œæƒ¯çœ‹ç§‹æœˆæ˜¥é£ã€‚ä¸€å£¶æµŠé…’å–œç›¸é€¢ï¼šå¤ä»Šå¤šå°‘äº‹ï¼Œéƒ½ä»˜ç¬‘è°ˆä¸­ã€‚2 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : è¯è¯´å¤©ä¸‹å¤§åŠ¿ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ï¼šå‘¨æœ«ä¸ƒå›½åˆ†äº‰ï¼Œå¹¶å…¥äºç§¦ã€‚åŠç§¦ç­ä¹‹åï¼Œæ¥šã€æ±‰åˆ†äº‰ï¼Œåˆå¹¶å…¥äºæ±‰ã€‚æ±‰æœè‡ªé«˜ç¥–æ–©ç™½è›‡è€Œèµ·ä¹‰ï¼Œä¸€ç»Ÿå¤©ä¸‹ã€‚åæ¥å…‰æ­¦ä¸­å…´ï¼Œä¼ è‡³çŒ®å¸ï¼Œé‚åˆ†ä¸ºä¸‰å›½ã€‚æ¨å…¶è‡´ä¹±ä¹‹ç”±ï¼Œæ®†å§‹äºæ¡“ã€çµäºŒå¸ã€‚æ¡“å¸ç¦é”¢å–„ç±»ï¼Œå´‡ ä¿¡å®¦å®˜ã€‚åŠæ¡“å¸å´©ï¼Œçµå¸å³ä½ï¼Œå¤§å°†å†›çª¦æ­¦ã€å¤ªå‚…é™ˆè•ƒï¼Œå…±ç›¸è¾…ä½ã€‚æ—¶æœ‰å®¦å®˜æ›¹èŠ‚ç­‰å¼„æƒï¼Œçª¦æ­¦ã€é™ˆè•ƒè°‹è¯›ä¹‹ï¼Œä½œäº‹ä¸å¯†ï¼Œåä¸ºæ‰€å®³ã€‚ä¸­æ¶“è‡ªæ­¤æ„ˆæ¨ªã€‚3 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å»ºå®äºŒå¹´å››æœˆæœ›æ—¥ï¼Œå¸å¾¡æ¸©å¾·æ®¿ã€‚æ–¹å‡åº§ï¼Œæ®¿è§’ç‹‚é£éª¤èµ·ï¼Œåª è§ä¸€æ¡å¤§é’è›‡ï¼Œä»æ¢ä¸Šé£å°†ä¸‹æ¥ï¼ŒèŸ äºæ¤…ä¸Šã€‚å¸æƒŠå€’ï¼Œå·¦å³æ€¥æ•‘å…¥å®«ï¼Œç™¾å®˜ä¿±å¥”é¿ã€‚é¡»è‡¾ï¼Œè›‡ä¸è§äº†ã€‚å¿½ç„¶å¤§é›·å¤§é›¨ï¼ŒåŠ ä»¥å†°é›¹ï¼Œè½åˆ°åŠå¤œæ–¹æ­¢ï¼Œåå´æˆ¿å±‹æ— æ•°ã€‚å»ºå®å››å¹´äºŒæœˆï¼Œæ´›é˜³åœ°éœ‡ï¼›åˆæµ·æ°´æ³›æº¢ï¼Œæ²¿æµ·å±…æ°‘ï¼Œå°½è¢«å¤§æµªå·å…¥æµ·ä¸­ã€‚å…‰å’Œå…ƒ å¹´ï¼Œé›Œé¸¡åŒ–é›„ã€‚å…­æœˆæœ”ï¼Œé»‘æ°”åé¦€ä¸ˆï¼Œé£å…¥æ¸©å¾·æ®¿ä¸­ã€‚ç§‹ä¸ƒæœˆï¼Œæœ‰è™¹è§äºç‰å ‚ï¼›äº”åŸå±±å²¸ï¼Œå°½çš†å´©è£‚ã€‚ç§ç§ä¸ç¥¥ï¼Œéæ­¢ä¸€ç«¯ã€‚4 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å¸ä¸‹è¯é—®ç¾¤è‡£ä»¥ç¾å¼‚ä¹‹ç”±ï¼Œè®®éƒè”¡é‚•ä¸Šç–ï¼Œä»¥ä¸ºéœ“å •é¸¡åŒ–ï¼Œä¹ƒå¦‡å¯ºå¹²æ”¿ä¹‹æ‰€è‡´ï¼Œè¨€é¢‡åˆ‡ ç›´ã€‚å¸è§ˆå¥å¹æ¯ï¼Œå› èµ·æ›´è¡£ã€‚æ›¹èŠ‚åœ¨åçªƒè§†ï¼Œæ‚‰å®£å‘Šå·¦å³Â·é‚ä»¥ä»–äº‹é™·é‚•äºç½ªï¼Œæ”¾å½’ç”°é‡Œã€‚åå¼ è®©ï¼Œèµµå¿ ï¼Œå°è«ï¼Œæ®µåœ­ï¼Œæ›¹èŠ‚ï¼Œå€™è§ˆï¼Œè¹‡ç¡•ï¼Œç¨‹æ—·ï¼Œå¤æ½ï¼Œéƒ­èƒœåäººæœ‹æ¯”ä¸ºå¥¸ï¼Œå·ä¸ºâ€œåå¸¸ä¾â€ã€‚å¸å°Šä¿¡å¼ è®©ï¼Œå‘¼ä¸ºâ€œé˜¿çˆ¶â€ï¼Œæœæ”¿æ—¥éï¼Œä»¥è‡´å¤©ä¸‹äººå¿ƒæ€ä¹±ï¼Œç›—è´¼èœ‚èµ·ã€‚5 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : æ—¶é’œé¹¿éƒ¡æœ‰å…„å¼Ÿä¸‰äººï¼šä¸€åå¼ è§’ï¼Œä¸€åå¼ å®ï¼Œä¸€åå¼ æ¢ã€‚é‚£å¼ è§’æœ¬æ˜¯ä¸ªä¸ç¬¬ç§€æ‰ã€‚å› å…¥å±±é‡‡è¯ï¼Œé‡ä¸€è€äººï¼Œç¢§çœ¼ç«¥é¢œï¼Œæ‰‹æ‰§è—œæ–ï¼Œå”¤è§’è‡³ä¸€æ´ä¸­ï¼Œä»¥å¤©ä¹¦ä¸‰å·æˆä¹‹ï¼Œæ›°ï¼šâ€œæ­¤åå¤ªå¹³è¦æœ¯ã€‚æ±å¾—ä¹‹ï¼Œå½“ä»£å¤©å®£åŒ–ï¼Œæ™®æ•‘ä¸–äººï¼›è‹¥èŒå¼‚å¿ƒï¼Œå¿…è·æ¶æŠ¥ã€‚â€è§’æ‹œé—®å§“åã€‚è€äººæ›°ï¼šâ€œå¾ä¹ƒå—åè€ä»™ä¹Ÿã€‚â€è¨€è®«ï¼ŒåŒ–é˜µæ¸…é£è€Œå»ã€‚6 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : è§’å¾—æ­¤ä¹¦ï¼Œæ™“å¤œæ”»ä¹ ï¼Œèƒ½å‘¼é£å”¤é›¨ï¼Œå·ä¸ºå¤ªå¹³é“äººã€‚ä¸­å¹³å…ƒå¹´æ­£æœˆå†…ï¼Œç–«æ°”æµè¡Œï¼Œå¼ è§’æ•£æ–½ç¬¦æ°´ï¼Œä¸ºäººæ²»ç—…ï¼Œè‡ªç§°å¤§è´¤è‰¯å¸ˆã€‚è§’æœ‰å¾’å¼Ÿäº”ç™¾é¦€äººï¼Œäº‘æ¸¸å››æ–¹ï¼Œçš†èƒ½ä¹¦ç¬¦å¿µå’’ã€‚æ¬¡åå¾’ä¼—æ—¥å¤šï¼Œè§’ä¹ƒç«‹ä¸‰åå…­æ–¹ï¼Œâ”€å¤§æ–¹ä¸‡é¦€äººï¼Œå°æ–¹å…­ä¸ƒåƒâ”€ï¼Œå„ç«‹æ¸ å¸…ï¼Œç§°ä¸ºå°†å†›ã€‚è®¹è¨€â€œè‹å¤©å·²æ­»ï¼Œé»„å¤©å½“ç«‹ã€‚â€åˆäº‘â€œå²åœ¨ç”²å­ï¼Œå¤©ä¸‹å¤§å‰ã€‚â€ä»¤äººå„ä»¥ç™½åœŸï¼Œä¹¦â€œç”²å­â€äºŒå­—äºå®¶ä¸­å¤§é—¨ä¸Šã€‚é’ã€å¹½ã€å¾ã€å†€ã€è†ã€æ‰¬ã€å…–ã€è±«å…«å·ä¹‹äººï¼Œå®¶å®¶ä¾å¥‰å¤§è´¤è‰¯å¸ˆå¼ è§’åå­—ã€‚è§’é£å…¶å…šé©¬å…ƒä¹‰ï¼Œæš—èµé‡‘å¸›ï¼Œç»“äº¤ä¸­æ¶“å°è«ï¼Œä»¥ä¸ºå†…åº”ã€‚è§’ä¸äºŒå¼Ÿå•†è®®æ›°ï¼šâ€œè‡³éš¾å¾—è€…ï¼Œæ°‘å¿ƒä¹Ÿã€‚ä»Šæ°‘å¿ƒå·²é¡ºï¼Œè‹¥ä¸ä¹˜åŠ¿å–å¤©ä¸‹ï¼Œè¯šä¸ºå¯æƒœã€‚â€é‚ä¸€é¢ç§é€ é»„æ——ï¼Œçº¦æœŸä¸¾äº‹ï¼›ä¸€é¢ä½¿å¼Ÿå­å”å·ï¼Œé©°ä¹¦æŠ¥å°è«ã€‚å”å·ä¹ƒè¿³èµ´çœä¸­å‘Šå˜ã€‚å¸å¬å¤§å°†å†›ä½•è¿›è°ƒå…µæ“’é©¬å…ƒä¹‰ï¼Œæ–©ä¹‹ï¼›æ¬¡æ”¶å°è«ç­‰ä¸€å¹²äººä¸‹ç‹±ã€‚7 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å¼ è§’é—»çŸ¥äº‹éœ²ï¼Œæ˜Ÿå¤œä¸¾å…µï¼Œè‡ªç§°å¤©å…¬å°†å†›ï¼Œâ”€å¼ å®ç§°åœ°å…¬å°†å†›ï¼Œå¼ æ¢ç§°äººå…¬å°†å†›â”€ã€‚ç”³è¨€äºä¼—æ›°ï¼šâ€œä»Šæ±‰è¿å°†ç»ˆï¼Œå¤§åœ£äººå‡ºï¼›æ±ç­‰çš†å®œé¡ºä»å¤©æ„ï¼Œä»¥æ¡¨å¤ªå¹³ã€‚â€å››æ–¹ç™¾å§“ï¼Œè£¹é»„å·¾ä»å¼ è§’åè€…ï¼Œå››äº”åä¸‡ã€‚è´¼åŠ¿æµ©å¤§ï¼Œå®˜å†›æœ›é£è€Œé¡ã€‚ä½•è¿›å¥å¸ç«é€Ÿé™è¯ï¼Œä»¤å„å¤„å¤‡å¾¡ï¼Œè®¨è´¼ç«‹åŠŸï¼›ä¸€é¢é£ä¸­éƒå°†å¢æ¤ï¼Œçš‡ç”«åµ©ï¼Œæœ±éš½ï¼Œå„å¼•ç²¾å…µï¼Œåˆ†ä¸‰è·¯è®¨ä¹‹ã€‚8 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : ä¸”è¯´å¼ è§’ä¸€å†›ï¼Œå‰çŠ¯å¹½å·ç•Œåˆ†ã€‚å¹½å·å¤ªå®ˆåˆ˜ç„‰ï¼Œä¹ƒæ±Ÿå¤ç«Ÿé™µäººæ°ï¼Œæ±‰é²æ­ç‹ä¹‹åä¹Ÿï¼›å½“æ—¶é—»å¾—è´¼å…µå°†è‡³ï¼Œå¬æ ¡å°‰é‚¹é–è®¡è®®ã€‚é–æ›°ï¼šâ€œè´¼å…µä¼—ï¼Œæˆ‘å…µå¯¡ï¼Œæ˜å…¬å®œä½œé€Ÿæ‹›å†›åº”æ•Œã€‚â€åˆ˜ç„‰ç„¶å…¶è¯´ï¼Œéš å³å‡ºæ¦œæ‹›å‹Ÿä¹‰å…µã€‚æ¦œæ–‡è¡Œåˆ°æ¶¿å¿ï¼Œä¹ƒå¼•å‡ºæ¶¿å¿ä¸­ä¸€ä¸ªè‹±é›„ã€‚9 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : é‚£äººä¸ç”šå¥½è¯»ä¹¦ï¼›æ€§å®½å’Œï¼Œå¯¡è¨€è¯­ï¼Œå–œæ€’ä¸å½¢äºè‰²ï¼›ç´ æœ‰å¤§å¿—ï¼Œä¸“å¥½ç»“äº¤å¤©ä¸‹è±ªæ°ï¼›ç”Ÿå¾—èº«é•¿ä¸ƒå°ºäº”å¯¸ï¼Œä¸¤è€³å‚è‚©ï¼ŒåŒæ‰‹è¿‡è†ï¼Œç›®èƒ½è‡ªé¡¾å…¶è€³ï¼Œé¢å¦‚å† ç‰ï¼Œå”‡è‹¥æ¶‚è„‚ï¼›ä¸­å±±é–ç‹åˆ˜èƒœä¹‹åï¼Œæ±‰æ™¯å¸é˜ä¸‹ç„å­™ï¼›å§“åˆ˜ï¼Œåå¤‡ï¼Œå­—ç„å¾·ã€‚æ˜”åˆ˜èƒœä¹‹å­åˆ˜è´ï¼Œæ±‰æ­¦æ—¶å°æ¶¿é¹¿äº­ä¾¯ï¼Œååé…¬é‡‘å¤±ä¾¯ï¼Œå› æ­¤é—è¿™ä¸€æåœ¨æ¶¿å¿ã€‚ç„å¾·ç¥–åˆ˜é›„ï¼Œçˆ¶åˆ˜å¼˜ã€‚å¼˜æ›¾ä¸¾å­å»‰ï¼Œäº¦å°ä½œåï¼Œæ—©ä¸§ã€‚ç„å¾·å¹¼å­¤ï¼Œäº‹æ¯è‡³å­ï¼›å®¶è´«ï¼Œè´©å±¦ ç»‡å¸­ä¸ºä¸šã€‚å®¶ä½æœ¬å¿æ¥¼æ¡‘æ‘ã€‚å…¶å®¶ä¹‹ä¸œå—ï¼Œæœ‰ä¸€å¤§æ¡‘æ ‘ï¼Œé«˜äº”ä¸ˆé¦€ï¼Œé¥æœ›ä¹‹ï¼Œç«¥ç«¥å¦‚è½¦ç›–ã€‚ç›¸è€…äº‘ï¼šâ€œæ­¤å®¶å¿…å‡ºè´µäººã€‚â€10 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : ç„å¾·å¹¼æ—¶ï¼Œä¸ä¹¡ä¸­å°å„¿æˆäºæ ‘ä¸‹ï¼Œæ›°ï¼šâ€œæˆ‘ä¸ºå¤©å­ï¼Œå½“ä¹˜æ­¤è½¦ç›–ã€‚â€å”çˆ¶åˆ˜å…ƒèµ·å¥‡å…¶è¨€ï¼Œæ›°ï¼šâ€œæ­¤å„¿éå¸¸äººä¹Ÿï¼â€å› è§ç„å¾·å®¶è´«ï¼Œå¸¸èµ„ç»™ä¹‹ã€‚å¹´åäº”å²ï¼Œæ¯ä½¿æ¸¸å­¦ï¼Œå°å¸ˆäº‹éƒ‘ç„ã€å¢æ¤ï¼›ä¸å…¬å­™ç“’ç­‰ä¸ºå‹ã€‚åŠåˆ˜ç„‰å‘æ¦œæ‹›å†›æ—¶ï¼Œç„å¾·å¹´å·±äºŒåå…«å²çŸ£ã€‚å½“æ—¥è§äº†æ¦œæ–‡ï¼Œæ…¨ç„¶é•¿å¹ã€‚éšåä¸€äººå‰å£°è¨€æ›°ï¼šâ€œå¤§ä¸ˆå¤«ä¸ä¸å›½å®¶å‡ºåŠ›ï¼Œä½•æ•…é•¿å¹ï¼Ÿâ€11 æ‰“å¼€å­— å…¸ å®´æ¡ƒå›­è±ª... : ç„å¾·å›è§†å…¶äººï¼šèº«é•¿å…«å°ºï¼Œè±¹å¤´ç¯çœ¼ï¼Œç‡•é¢”è™é¡»ï¼Œå£°è‹¥å·¨é›·ï¼ŒåŠ¿å¦‚å¥”é©¬ã€‚ç„å¾·è§ä»–å½¢è²Œå¼‚å¸¸ï¼Œé—®å…¶å§“åã€‚å…¶äººæ›°ï¼šâ€œæŸå§“å¼ ï¼Œåé£ï¼Œå­—ç¿¼å¾·ã€‚ä¸–å±…æ¶¿éƒ¡ï¼Œé¢‡æœ‰åº„ç”°ï¼Œå–é…’å± çŒªï¼Œä¸“å¥½ç»“äº¤å¤©ä¸‹è±ªæ°ã€‚é€‚æ‰è§å…¬çœ‹æ¦œè€Œå¹ï¼Œæ•…æ­¤ç›¸ é—®ã€‚â€ç„å¾·æ›°ï¼šâ€œæˆ‘æœ¬æ±‰å®¤å®—äº²ï¼Œå§“åˆ˜ï¼Œåå¤‡ã€‚ä»Šé—»é»„å·¾å€¡ä¹±ï¼Œæœ‰å¿—æ¬²ç ´è´¼å®‰æ°‘ï¼›æ¨åŠ›ä¸èƒ½ï¼Œæ•…é•¿å¹è€³ã€‚â€é£æ›°ï¼šâ€œå¾é¢‡æœ‰èµ„è´¢ï¼Œå½“æ‹›å‹Ÿä¹¡å‹‡ï¼Œä¸å…¬åŒä¸¾å¤§äº‹ï¼Œå¦‚ä½•ï¼Ÿâ€ç„å¾·ç”šå–œï¼Œé‚ä¸åŒå…¥æ‘åº—ä¸­é¥®é…’ã€‚12 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : æ­£é¥®é—´ï¼Œè§ä¸€å¤§æ±‰ï¼Œæ¨è‘—ä¸€è¾†è½¦å­ï¼Œåˆ°åº—é—¨é¦–æ­‡äº†ï¼›å…¥åº—åä¸‹ï¼Œä¾¿å”¤é…’ä¿ï¼šâ€œå¿«æ–Ÿé…’æ¥åƒï¼Œæˆ‘å¾…èµ¶å…¥åŸå»æŠ•å†›ã€‚â€ç„å¾·çœ‹å…¶äººï¼šèº«é•¿ä¹å°ºï¼Œé«¯é•¿äºŒå°ºï¼šé¢å¦‚é‡æ£ï¼Œå”‡è‹¥æ¶‚è„‚ï¼›ä¸¹å‡¤çœ¼ï¼Œå§èš•çœ‰ï¼šç›¸è²Œå ‚å ‚ï¼Œå¨é£å‡›å‡›ã€‚ç„å¾·å°±é‚€ä»–åŒåï¼Œå©å…¶å§“åã€‚å…¶äººæ›°ï¼šâ€œå¾å§“å…³ï¼Œåç¾½ï¼Œå­—å¯¿é•¿ï¼Œåæ”¹äº‘é•¿ï¼Œæ²³ä¸œè§£è‰¯äººä¹Ÿã€‚å› æœ¬å¤„åŠ¿è±ªï¼Œå€šåŠ¿å‡Œäººï¼Œè¢«å¾æ€äº†ï¼›é€ƒéš¾æ±Ÿæ¹–ï¼Œäº”å…­å¹´çŸ£ã€‚ä»Šé—»æ­¤å¤„æ‹›å†›ç ´è´¼ï¼Œç‰¹æ¥åº”å‹Ÿã€‚â€ç„å¾·é‚ä»¥å·±å¿—å‘Šä¹‹ã€‚äº‘é•¿å¤§å–œã€‚åŒåˆ°å¼ é£åº„ä¸Šï¼Œå…±è®®å¤§äº‹ã€‚13 æ‰“å¼€å­—å…¸ç›¸å…³è®¨è®º å®´æ¡ƒå›­è±ª... : é£æ›°ï¼šâ€œå¾åº„åæœ‰ä¸€æ¡ƒå›­ï¼ŒèŠ±å¼€æ­£ç››ï¼›æ˜æ—¥å½“äºå›­ä¸­ç¥­å‘Šå¤©åœ°ï¼Œæˆ‘ä¸‰äººç»“ä¸ºå…„å¼Ÿï¼ŒååŠ›åŒå¿ƒï¼Œç„¶åå¯å›¾å¤§äº‹ã€‚â€ç„å¾·ã€äº‘é•¿ã€é½å£°åº”æ›°ï¼šâ€œå¦‚æ­¤ç”šå¥½ã€‚â€æ¬¡æ—¥ï¼Œäºæ¡ƒå›­ä¸­ï¼Œå¤‡ä¸‹ä¹Œç‰›ç™½é©¬ç¥­ç¤¼ç­‰é¡¹ï¼Œä¸‰äººç„šé¦™ï¼Œå†æ‹œè€Œè¯´èª“æ›°ï¼šâ€œå¿µåˆ˜å¤‡ã€å…³ç¾½ã€å¼ é£ï¼Œè™½ç„¶å¼‚å§“ï¼Œæ—¢ç»“ä¸ºå…„å¼Ÿï¼Œåˆ™åŒå¿ƒååŠ›ï¼Œæ•‘å›°æ‰¶å±ï¼›ä¸ŠæŠ¥å›½å®¶ï¼Œä¸‹å®‰é»åº¶ï¼›ä¸æ±‚åŒå¹´åŒæœˆåŒæ—¥ç”Ÿï¼Œä½†æ„¿åŒå¹´åŒæœˆåŒæ—¥æ­»ã€‚çš‡å¤©ååœŸï¼Œå®é‰´æ­¤å¿ƒã€‚èƒŒä¹‰å¿˜æ©ï¼Œå¤©äººå…±æˆ®ã€‚â€èª“æ¯•ï¼Œæ‹œç„å¾·ä¸ºå…„ï¼Œå…³ç¾½æ¬¡ä¹‹ï¼Œå¼ é£ä¸ºå¼Ÿã€‚ç¥­ç½¢å¤©åœ°ï¼Œå¤å®°ç‰›è®¾é…’ï¼Œèšä¹¡ä¸­å‹‡å£«ï¼Œå¾—ä¸‰ç™¾é¦€äººï¼Œå°±æ¡ƒå›­ä¸­ç—›é¥®ä¸€é†‰ã€‚æ¥æ—¥æ”¶æ‹¾å†›å™¨ï¼Œä½†æ¨æ— é©¬åŒ¹å¯ä¹˜ã€‚14 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : æ­£æ€è™‘é—´ï¼ŒäººæŠ¥â€œæœ‰ä¸¤ä¸ªå®¢äººï¼Œå¼•ä¸€å¤¥ä¼´å½“ï¼Œèµ¶ä¸€ç¾¤é©¬ï¼ŒæŠ•åº„ä¸Šæ¥ã€‚â€ç„å¾·æ›°ï¼šâ€œæ­¤å¤©ä½‘æˆ‘ä¹Ÿï¼â€ä¸‰äººå‡ºåº„è¿æ¥ã€‚åŸæ¥äºŒå®¢ä¹ƒä¸­å±±å¤§å•†ï¼šä¸€åå¼ ä¸–å¹³ï¼Œä¸€åè‹åŒï¼Œæ¯å¹´å¾€åŒ—è´©é©¬ï¼Œè¿‘å› å¯‡å‘è€Œå›ã€‚ç„å¾·è¯·äºŒäººåˆ°åº„ï¼Œç½®é…’ç®¡å¾…ï¼Œè¯‰è¯´æ¬²è®¨è´¼å®‰æ°‘ä¹‹æ„ã€‚äºŒå®¢å¤§å–œï¼Œæ„¿å°†è‰¯é©¬äº”ååŒ¹ç›¸é€ï¼›åˆèµ é‡‘é“¶äº”ç™¾ä¸¤ï¼Œé•”é“ä¸€åƒæ–¤ï¼Œä»¥èµ„å™¨ç”¨ã€‚ç„å¾·è°¢åˆ«äºŒå®¢ï¼Œä¾¿å‘½è‰¯åŒ æ‰“é€ åŒè‚¡å‰‘ã€‚äº‘é•¿é€ é’é¾™åƒæœˆåˆ€ï¼Œåˆåå†·è‰³ é”¯ï¼Œé‡å…«åäºŒæ–¤ã€‚å¼ é£é€ ä¸ˆå…«ç‚¹é’¢çŸ›ã€‚å„ç½®å…¨èº«é“ ç”²ã€‚å…±èšä¹¡å‹‡äº”ç™¾é¦€äººï¼Œæ¥è§é‚¹é–ã€‚é‚¹é–å¼•è§å¤ªå®ˆåˆ˜ç„‰ã€‚ä¸‰äººå‚è§æ¯•ï¼Œå„é€šå§“åã€‚ç„å¾·è¯´èµ·å®—æ´¾ï¼Œåˆ˜ç„‰å¤§å–œï¼Œé‚è®¤ç„å¾·ä¸ºä¾„ã€‚
"""

def generate_content_sync():
    response = client.models.generate_content(
        model="gemini-2.5-flash-preview-05-20",
        contents=prompt + "ä¸‰å›½æ¼”ä¹‰è¿™ä¸€å›å‡ºç°äº†å‡ ä¸ªä»»åŠ¡ ",
    )
    print(response.usage_metadata)  # å‘½ä¸­ç¼“å­˜æ—¶ä¼šæ˜¾ç¤º cache_tokens_details å’Œ cached_content_token_count å­—æ®µ
    return response

generate_content_sync()
```

> å‘½ä¸­ç¼“å­˜æ—¶ï¼Œ`response.usage_metadata` ä¼šåŒ…å«å¦‚ä¸‹ç»“æ„ï¼š
>
> ```
> cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=2003)]
> cached_content_token_count=2003
> ```

**æ ¸å¿ƒç»“è®º**ï¼šéšå¼ç¼“å­˜æ”¯æŒè‡ªåŠ¨å‘½ä¸­ä¸å‘½ä¸­åé¦ˆã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡ usage_metadata åˆ¤æ–­å‘½ä¸­æƒ…å†µã€‚æˆæœ¬èŠ‚çœéä¿è¯ï¼Œå®é™…æ•ˆæœå› è¯·æ±‚ç»“æ„å’Œä½¿ç”¨åœºæ™¯è€Œå¼‚ã€‚

## Function calling

ä½¿ç”¨ openai å…¼å®¹æ–¹å¼è°ƒç”¨ Gemini çš„ function calling åŠŸèƒ½æ—¶ï¼Œéœ€è¦åœ¨è¯·æ±‚ä½“å†…éƒ¨ä¼ å…¥`tool_choice="auto"`ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚

<CodeGroup>

```py Python
from openai import OpenAI

# Define the function declaration for the model
schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# Configure the client
client = OpenAI(
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

# Send request with function declarations using OpenAI compatible format
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
        {"role": "user", "content": "Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning."}
    ],
    tools=[{"type": "function", "function": schedule_meeting_function}],
    tool_choice="auto" ## ğŸ“ æ­¤å¤„è¿½åŠ äº† Aihubmix å…¼å®¹ï¼Œæ›´ç¨³å®šçš„è¯·æ±‚æ–¹å¼
)

# Check for a function call
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    function_call = tool_call.function
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.arguments}")
    print(response.usage)
    #  In a real app, you would call your function here:
    #  result = schedule_meeting(**json.loads(function_call.arguments))
else:
    print("No function call found in the response.")
    print(response.choices[0].message.content)
```

</CodeGroup>

**è¾“å‡ºç»“æœç¤ºä¾‹ï¼š**

```bash
Function to call: schedule_meeting
Arguments: {"attendees":["Bob","Alice"],"date":"2025-03-14","time":"10:00","topic":"Q3 planning"}
CompletionUsage(completion_tokens=28, prompt_tokens=111, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None)
```

## Tokens ç”¨é‡è¿½è¸ª

1. Gemini åŸç”Ÿé‡‡ç”¨ `usage_metadata` æ¥[è¿½è¸ªä½¿ç”¨çš„ token](https://ai.google.dev/gemini-api/docs/tokens?lang=python)ï¼Œå…¶ä¸­çš„å­—æ®µå¯¹åº”å¦‚ä¸‹ï¼š

- prompt_token_count: è¾“å…¥ token æ•°
- candidates_token_count: è¾“å‡º token æ•°
- thoughts_token_count: æ¨ç†ä½¿ç”¨çš„ token æ•°ï¼Œæ€§è´¨ä¸Šä¹Ÿæ˜¯è¾“å‡º token
- total_token_count: æ€» token ä½¿ç”¨é‡ï¼ˆè¾“å…¥\+è¾“å‡ºï¼‰

2. å¯¹äº OpenAI å…¼å®¹æ ¼å¼ï¼Œåˆ™é‡‡ç”¨ `.usage` æ¥è¿½è¸ªï¼Œå­—æ®µå¯¹åº”å¦‚ä¸‹ï¼š

- usage.completion_tokens: è¾“å…¥ token æ•°
- usage.prompt_tokens: è¾“å‡º token æ•°ï¼ˆåŒ…å«æ¨ç†ä½¿ç”¨çš„ token æ•°ï¼‰
- usage.total_tokens:æ€» token ä½¿ç”¨é‡

**ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹:**

<CodeGroup>

```py Gemini åŸç”Ÿ
from google import genai
from google.genai import types
import time

def generate():
    client = genai.Client(
        api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-03-25"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""é‡‘èé¢†åŸŸçš„ã€Œ72 æ³•åˆ™ã€æ˜¯å¦‚ä½•æ¨å¯¼çš„ï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    final_usage_metadata = None
    
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")
        if chunk.usage_metadata:
            final_usage_metadata = chunk.usage_metadata
    
    # åœ¨æ‰€æœ‰ chunk å¤„ç†å®Œåï¼Œæ‰“å°å®Œæ•´çš„ token ä½¿ç”¨æƒ…å†µ
    if final_usage_metadata:
        print(f"\nUsage: {final_usage_metadata}")

if __name__ == "__main__":
    generate()
```


```py OpenAI å…¼å®¹
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17",
    reasoning_effort="low", #"low", "medium", and "high", which behind the scenes we map to 1K, 8K, and 24K thinking token budgets. If you want to disable thinking, you can set the reasoning effort to "none".
    messages=[
        {
            "role": "user",
            "content": "é‡‘èé¢†åŸŸçš„ã€Œ72 æ³•åˆ™ã€æ˜¯å¦‚ä½•æ¨å¯¼çš„ï¼Ÿ"
        }
    ],
    stream=True
)

#print(completion.choices[0].message.content)

for chunk in completion:
    print(chunk.choices[0].delta)
    # åªåœ¨æœ€åä¸€ä¸ª chunkï¼ˆåŒ…å«å®Œæ•´ usage æ•°æ®ï¼‰æ—¶æ‰“å° usage ä¿¡æ¯
    if chunk.usage and chunk.usage.completion_tokens > 0:
        print(f"è¾“å‡º tokens: {chunk.usage.completion_tokens}")
        print(f"è¾“å…¥ tokens: {chunk.usage.prompt_tokens}")
        print(f"æ€» tokens: {chunk.usage.total_tokens}")
```

</CodeGroup>