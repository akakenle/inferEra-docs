---
title: "é˜¿é‡Œé€šä¹‰ç³»åˆ—"
icon: "a"
---

## Qwen 3 ç³»åˆ—

Qwen3 ç³»åˆ—æ˜¯é˜¿é‡Œæ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§æ¨¡å‹ï¼Œèƒ½åŠ›å¤§å¹…è·ƒå‡ï¼šåœ¨ä»£ç ç†è§£ã€æ•°å­¦æ¨ç†ã€å¤šè¯­è¨€è¡¨è¾¾ã€å¤æ‚æ¨æ–­ä»»åŠ¡ä¸Šï¼Œæ¯”è‚©ç”šè‡³è¶…è¶Šäº†ç›®å‰å¸‚é¢ä¸Šçš„é¡¶çº§æ¨¡å‹ï¼ˆå¦‚ o1ã€DeepSeek-R1ï¼‰ã€‚**å®ƒçš„æ ¸å¿ƒçªç ´åœ¨äºå¼•å…¥äº†ã€Œæ€è€ƒæ¨¡å¼ã€ä¸ã€Œéæ€è€ƒæ¨¡å¼ã€åˆ‡æ¢æœºåˆ¶ï¼Œè®©æ¨¡å‹åœ¨é¢å¯¹ä¸åŒéš¾åº¦ä»»åŠ¡æ—¶ï¼Œè‡ªä¸»è°ƒèŠ‚æ¨ç†æ·±åº¦ï¼Œå®ç°äº†é€Ÿåº¦ä¸ç²¾åº¦çš„åŒä¼˜å¹³è¡¡ã€‚** æ——èˆ°ç‰ˆ Qwen3-235B é‡‡ç”¨ç¨€ç–æ¿€æ´»ï¼Œä»…ç”¨ 22B å‚æ•°æ¨ç†ï¼Œå…¼é¡¾æˆæœ¬å’Œå“è¶Šèƒ½åŠ›ã€‚å…¨ç³»æ¨¡å‹å…¨é¢å¼€æºï¼Œæ¶µç›–ä»è½»é‡åˆ°è¶…å¤§è§„æ¨¡éœ€æ±‚ã€‚

**1. åŸºç¡€ç”¨æ³•ï¼š** ç”¨ OpenAI å…¼å®¹æ ¼å¼è½¬å‘ã€‚  
**2. å·¥å…·è°ƒç”¨ï¼š** å¸¸è§„ Tools è°ƒç”¨æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼ï¼ˆé€‚ç”¨äº V2.5ã€V3ï¼‰ï¼Œè€Œ MCP Tools ä¾èµ– `qwen-agent`ï¼Œéœ€è¦å…ˆè¿è¡ŒæŒ‡ä»¤å®‰è£…ä¾èµ–ï¼š`pip install -U qwen-agent mcp`ã€‚
æ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒ[é˜¿é‡Œå®˜æ–¹æ–‡æ¡£](https://huggingface.co/Qwen/Qwen3-235B-A22B)

<CodeGroup>

```py åŸºç¡€ç”¨æ³•
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="Qwen/Qwen3-30B-A3B",
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ],
    stream=True
)

# æŸäº› chunk å¯¹è±¡å¯èƒ½æ²¡æœ‰ choices å±æ€§æˆ– choices æ˜¯ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå¤„ç†æ–¹æ³•ï¼š
for chunk in completion:
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```

```py Tools
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "è·å–æŒ‡å®šä½ç½®çš„å½“å‰å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œå¦‚åŒ—äº¬ã€ä¸Šæµ·ç­‰"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æ¸©åº¦å•ä½"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚ï¼ŒåŒ…å«å·¥å…·å®šä¹‰
completion = client.chat.completions.create(
    model="Qwen/Qwen3-30B-A3B", #2.5 å’Œ 3 éƒ½æ”¯æŒï¼ŒQwQ ä¸æ”¯æŒ
    messages=[
        {
            "role": "user",
            "content": "åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        }
    ],
    tools=tools,
    tool_choice="auto",  # è®©æ¨¡å‹è‡ªè¡Œå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    stream=True
)

# ç”¨äºæ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯çš„å­—å…¸
tool_calls = {}

# å¤„ç†æµå¼å“åº”
for chunk in completion:
    if not hasattr(chunk.choices, '__len__') or len(chunk.choices) == 0:
        continue
        
    delta = chunk.choices[0].delta
    
    # å¤„ç†æ–‡æœ¬å†…å®¹
    if hasattr(delta, 'content') and delta.content:
        print(delta.content, end="")
    
    # å¤„ç†å·¥å…·è°ƒç”¨
    if hasattr(delta, 'tool_calls') and delta.tool_calls:
        for tool_call in delta.tool_calls:
            if not hasattr(tool_call, 'index'):
                continue
                
            idx = tool_call.index
            if idx not in tool_calls:
                tool_calls[idx] = {"name": "", "arguments": ""}
                
            if hasattr(tool_call, 'function'):
                if hasattr(tool_call.function, 'name') and tool_call.function.name:
                    tool_calls[idx]["name"] = tool_call.function.name
                if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:
                    tool_calls[idx]["arguments"] += tool_call.function.arguments

# å®Œæˆåï¼Œæ‰“å°æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
for idx, info in tool_calls.items():
    if info["name"]:
        print(f"\nå·¥å…·è°ƒç”¨ï¼š{info['name']}")
    if info["arguments"]:
        print(f"å‚æ•°ï¼š{info['arguments']}")

```

```py MCP Tools
from qwen_agent.agents import Assistant
import os

# Define LLM
llm_cfg = {
    'model': 'Qwen/Qwen3-30B-A3B',

    # Use a custom endpoint compatible with OpenAI API:
    'model_server': 'https://aihubmix.com/v1',
    'api_key': os.getenv('AIHUBMIX_API_KEY'),

    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define Tools
tools = [
    {'mcpServers': {  # You can specify the MCP configuration file
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            "fetch": {
                "command": "uvx",
                "args": ["mcp-server-fetch"]
            }
        }
    },
  'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)
```

</CodeGroup>

## QvQã€Qwen 2.5 å’Œ QwQ ç³»åˆ—

ç”¨ OpenAI çš„å…¼å®¹æ ¼å¼è½¬å‘å³å¯ï¼ŒåŒºåˆ«åœ¨äºæµå¼è°ƒç”¨çš„æå–ï¼Œéœ€è¦å‰”é™¤ä¸ºç©ºçš„ `chunk.choices[0].delta.content`ï¼Œå‚è€ƒå¦‚ä¸‹ã€‚

**1. QvQã€Qwen 2.5 VLï¼š** å›¾ç‰‡è¯†åˆ«  
**2. QwQï¼š** æ–‡æœ¬ä»»åŠ¡  

<Info>
  `Qwen/QVQ-72B-Preview` æ˜¯åŸºäº `Qwen2-VL-72B` æ„å»ºçš„å¼€æºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä¸“æ³¨äºè§†è§‰æ¨ç†å’Œè·¨æ¨¡æ€ä»»åŠ¡ã€‚
</Info>

<CodeGroup>

```py Qwen 2.5 VL
from openai import OpenAI
import base64
import os

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

image_path = "yourpath/file.png"

# è¯»å–å¹¶ç¼–ç å›¾ç‰‡
def encode_image(image_path):
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{image_path}")
    
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# è·å–å›¾ç‰‡çš„ base64 ç¼–ç 
base64_image = encode_image(image_path)

# åˆ›å»ºåŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„æ¶ˆæ¯
completion = client.chat.completions.create(
    model="qwen2.5-vl-72b-instruct", #qwen2.5-vl-72b-instruct æˆ– Qwen/QVQ-72B-Preview
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬å›¾ç‰‡ä¸­çš„å†…å®¹ã€é£æ ¼å’Œå¯èƒ½çš„å«ä¹‰ã€‚"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{base64_image}"
                    }
                }
            ]
        }
    ],
    stream=True
)

for chunk in completion:
    # å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```


```py QwQ
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ¢æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é’¥
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="Qwen/QwQ-32B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æ”¯é…å®‡å®™çš„å…ƒè§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ"}
            ]
        }
    ],
    stream=True
)

for chunk in completion:
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```

</CodeGroup>