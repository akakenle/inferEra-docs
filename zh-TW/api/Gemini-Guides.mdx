---
title: "Gemini æŒ‡å—"
description: "é›™å­æ˜Ÿæ¼«éŠæŒ‡å—ï¼šæœ¬ç«™ Gemini èª¿ç”¨ç´°ç¯€ç¸½è¦½ã€‚"
icon: "google"
---

## Gemini èª¿ç”¨æ–¹å¼

å°æ–¼ Gemini ç³»åˆ—ï¼Œæˆ‘å€‘æä¾›åŸç”Ÿèª¿ç”¨å’Œ Openai ç›¸å®¹é€™ 2 ç¨®èª¿ç”¨æ–¹å¼ã€‚\
ä½¿ç”¨å‰è«‹åŸ·è¡Œ `pip install google-genai` æˆ– `pip install -U google-genai`ï¼Œå®‰è£ï¼ˆæ›´æ–°ï¼‰åŸç”Ÿä¾è³´ã€‚

1ï¸âƒ£ åŸç”Ÿèª¿ç”¨æ™‚ï¼Œéœ€åœ¨å…§éƒ¨å‚³å…¥ AiHubMix å¯†é‘°å’Œè«‹æ±‚é€£çµã€‚æ³¨æ„é€™å€‹é€£çµå’Œå¸¸è¦ `base_url` å¯«æ³•ä¸åŒï¼Œè«‹åƒè€ƒç¯„ä¾‹ï¼š

```py
client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"},
)
```

2ï¸âƒ£ Openai ç›¸å®¹æ ¼å¼å‰‡ç¶­æŒé€šç”¨çš„ `v1` ç«¯é»ï¼š

```py
client = OpenAI(
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)
```

3ï¸âƒ£ é‡å° 2.5 ç³»åˆ—ï¼Œå¦‚éœ€é¡¯ç¤ºæ¨ç†éç¨‹ï¼Œå¯ç”¨ä»¥ä¸‹ 2 ç¨®æ–¹å¼ï¼š

1. åŸç”Ÿèª¿ç”¨ï¼šå‚³å…¥ `include_thoughts=True`
2. OpenAI ç›¸å®¹æ–¹å¼ï¼šå‚³å…¥ `reasoning_effort`

è©³ç´°èª¿ç”¨è«‹åƒè€ƒä¸‹æ–‡ç¨‹å¼ç¢¼ç¯„ä¾‹ã€‚

### Gemini 2.5 ç³»åˆ—ã€Œæ¨ç†ã€èªªæ˜

1. 2.5 ç³»åˆ—çš†ç‚ºæ¨ç†æ¨¡å‹ã€‚
2. 2.5 flash ç‚ºæ··åˆæ¨¡å‹ï¼Œé¡ä¼¼ claude sonnet 3.7ï¼Œå¯ç”¨ `thinking_budget` æ§åˆ¶æ¨ç†é ç®—ä»¥é”æœ€ä½³æ•ˆæœã€‚
3. 2.5 pro ç‚ºç´”æ¨ç†æ¨¡å‹ï¼Œç„¡æ³•é—œé–‰ thinkingï¼Œä¹Ÿä¸é¡¯å¼å‚³éæ¨ç†é ç®—ã€‚

**Python èª¿ç”¨åƒè€ƒå¦‚ä¸‹ï¼š**

<CodeGroup>

```py æ™®é€šéæµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å°æ–¼æ™®é€šè‚¡ç¥¨æŠ•è³‡è€…ï¼šåˆ†æè²¡å ±æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]

    print(client.models.generate_content(
        model=model,
        contents=contents,
    ))

if __name__ == "__main__":
    generate()
```


```py 2.0 ç³»åˆ—-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å°æ–¼æ™®é€šè‚¡ç¥¨æŠ•è³‡è€…ï¼šåˆ†æè²¡å ±æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py 2.5 Flash-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-flash-preview-04-17" #gemini-2.5-pro-preview-03-25ã€gemini-2.5-flash-preview-04-17
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""å°æ–¼æ™®é€šè‚¡ç¥¨æŠ•è³‡è€…ï¼šåˆ†æè²¡å ±æœ‰ç”¨çš„è¯ï¼Œè¿˜è¦è¿æ°”åšä»€ä¹ˆï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        thinking_config = types.ThinkingConfig(
            thinking_budget=2048, #ç¯„åœ 0-16384ã€‚é»˜èª 1024ï¼Œæœ€ä½³é‚Šéš›æ•ˆæœ 16000
        ),
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py 2.5 Pro-æµå¼
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-03-25"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""æ€ä¹ˆçŸ¥é“æˆ‘ä¸æ˜¯åœ¨æµªè´¹æ—¶é—´"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```


```py æ˜¾ç¤ºæ¨ç†å†…å®¹
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-05-06"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""é‡‘èé¢†åŸŸçš„ã€Œ72 æ³•åˆ™ã€æ˜¯å¦‚ä½•æ¨å¯¼çš„ï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
        thinking_config=types.ThinkingConfig(
            include_thoughts=True  # ğŸ§  å•Ÿç”¨æ€è€ƒéç¨‹è¼¸å‡º
        ),
    )

    # ç”¨æ–¼å„²å­˜æœ€å¾Œä¸€å€‹ chunk çš„ usage_metadata
    final_usage_metadata = None
    
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        # æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹éƒ¨åˆ†
        if chunk.candidates and len(chunk.candidates) > 0:
            for part in chunk.candidates[0].content.parts:
                if part.text:
                    if part.thought:
                        # æ€è€ƒéç¨‹å…§å®¹
                        print(part.text, end="")
                    else:
                        # æœ€çµ‚ç­”æ¡ˆå…§å®¹
                        print(part.text, end="")
        
        # ä¿å­˜æœ€æ–°çš„ usage_metadataï¼Œåªæœ‰æœ€å¾Œä¸€å€‹ chunk æœƒåŒ…å«å®Œæ•´ä¿¡æ¯
        if chunk.usage_metadata:
            final_usage_metadata = chunk.usage_metadata
    
    # åœ¨æ‰€æœ‰ chunk è™•ç†å®Œå¾Œï¼Œæ‰“å°å®Œæ•´çš„ token ä½¿ç”¨æƒ…æ³
    if final_usage_metadata:
        print(f"\n\nğŸ“Š Token ä½¿ç”¨æƒ…æ³:")
        print(f"æ€è€ƒ tokens: {getattr(final_usage_metadata, 'thoughts_token_count', 'ä¸å¯ç”¨')}")
        print(f"è¼¸å‡º tokens: {getattr(final_usage_metadata, 'candidates_token_count', 'ä¸å¯ç”¨')}")
        print(f"ç¸½è¨ˆ: {final_usage_metadata}")

if __name__ == "__main__":
    generate()
```

</CodeGroup>

## Gemini 2.5 Flash æ”¯æŒ

Openai ç›¸å®¹æ–¹å¼èª¿ç”¨åƒè€ƒå¦‚ä¸‹ï¼š

<CodeGroup>

```py Python ç”¨æ–¼å¿«é€Ÿä»»å‹™æ™‚ï¼Œé—œé–‰æ€è€ƒ
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17-nothink",
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ]
)

print(completion.choices[0].message.content)
```


```py Python æ§åˆ¶é ç®—
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17",
    reasoning_effort="low", # å¯é¸ "low", "medium" å’Œ "high", åˆ†åˆ¥å°æ‡‰ 1024, 8192 å’Œ 16384 æ¨ç†é ç®—
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ]
)

print(completion.choices[0].message.content)
```


```shell Curl-åŸºç¤èª¿ç”¨
curl -X POST https://aihubmix.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-***" \
  -d '{
    "model": "gemini-2.5-flash-preview-04-17-nothink",
    "messages": [
      {
        "role": "user",
        "content": "Explain the Occam'\''s Razor concept and provide an everyday example of it."
      }
    ]
  }'
```


```shell Curl-Thinking é¡¯ç¤º
curl https://aihubmix.com/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer sk-***" \
-d '{
  "model": "gemini-2.5-pro-preview-05-06",
  "messages": [
    {
      "role": "user",
      "content": "Explain the Occam'\''s Razor concept and provide an everyday example of it."
    }
  ],
  "reasoning_effort": "low"
}'
```

</CodeGroup>

<Tip>
  1. ç”¨æ–¼è¤‡é›œä»»å‹™æ™‚ï¼Œåªéœ€è¦å°‡æ¨¡å‹ id è¨­ç½®ç‚ºé»˜èªé–‹å•Ÿæ€è€ƒçš„ `gemini-2.5-flash-preview-04-17` å³å¯ã€‚
  2. Gemini 2.5 Flash é€šé `budget`ï¼ˆæ€è€ƒé ç®—ï¼‰ä¾†æ§åˆ¶æ€è€ƒçš„æ·±åº¦ï¼Œç¯„åœ 0-16Kï¼Œç›®å‰è½‰ç™¼é‡‡ç”¨çš„æ˜¯é»˜èªé ç®— 1024ï¼Œæœ€ä½³é‚Šéš›æ•ˆæœç‚º 16Kã€‚
</Tip>

## å¤šåª’é«”æ–‡ä»¶

Aihubmix ç›®å‰åªæ”¯æŒ**å°æ–¼ 20MB** çš„å¤šåª’é«”æ–‡ä»¶ï¼ˆåœ–ç‰‡ã€éŸ³é »ã€è¦–é »ï¼‰ï¼Œç”¨ `inline_data` ä¸Šå‚³ã€‚\
å¤§æ–¼ 20M çš„å¤šåª’é«”éœ€è¦ç”¨ File APIï¼ˆå°šæœªæ”¯æŒï¼‰ï¼Œå¾…å®Œå–„ç‹€æ…‹è·Ÿè¸ªï¼Œè¿”å› `upload_url`ã€‚

<CodeGroup>

```py åœ–ç‰‡
from google import genai
from google.genai import types

# è®€å–æ–‡ä»¶ç‚ºäºŒé€²åˆ¶æ•¸æ“š
file_path = "yourpath/file.jpeg"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="image/jpeg"
                )
            ),
            types.Part(
                text="Describe the image."
            )
        ]
    )
)

print(response.text)
```


```py éŸ³é¢‘
from google import genai
from google.genai import types

# è®€å–æ–‡ä»¶ç‚ºäºŒé€²åˆ¶æ•¸æ“š
file_path = "yourpath/file.m4a"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="audio/m4a"
                )
            ),
            types.Part(
                text="Transcribe the audio to text."
            )
        ]
    )
)

print(response.text)
```


```py è§†é¢‘
from google import genai
from google.genai import types

# è®€å–æ–‡ä»¶ç‚ºäºŒé€²åˆ¶æ•¸æ“š
file_path = "yourpath/file.mp4"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="video/mp4"
                )
            ),
            types.Part(
                text="Summarize this video. Then create a quiz with an answer key based on the information in this video."
            )
        ]
    )
)

print(response.text)
```


```py Youtube é“¾æ¥
from google import genai
from google.genai import types

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(
                    file_uri="https://www.youtube.com/watch?v=OoU7PwNyYUw"
                )
            ),
            types.Part(
                text="Please summarize the video in 3 sentences."
            )
        ]
    )
)

print(response.text)
```

</CodeGroup>

## Code Execution

è‡ªåŠ¨ä»£ç è§£æå™¨ç”¨ä¾‹å‚è€ƒï¼š

```py Python
from google import genai
from google.genai import types

# è®€å–æ–‡ä»¶ç‚ºäºŒé€²åˆ¶æ•¸æ“š
file_path = "yourpath/file.csv"
with open(file_path, "rb") as f:
    file_bytes = f.read()

client = genai.Client(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    http_options={"base_url": "https://aihubmix.com/gemini"}
)

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=file_bytes,
                    mime_type="text/csv"
                )
            ),
            types.Part(
                text="Please analyze this CSV and summarize the key statistics. Use code execution if needed."
            )
        ]
    ),
    config=types.GenerateContentConfig(
        tools=[types.Tool(
            code_execution=types.ToolCodeExecution
        )]
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if getattr(part, "executable_code", None) is not None:
        print("Generated code:\n", part.executable_code.code)
    if getattr(part, "code_execution_result", None) is not None:
        print("Execution result:\n", part.code_execution_result.output)
```

## ä¸Šä¸‹æ–‡å¿«å–

Gemini åœ¨åŸç”Ÿ API ä¸‹é è¨­å•Ÿç”¨äº†**éš±å¼ä¸Šä¸‹æ–‡å¿«å–**ï¼Œç„¡éœ€é–‹ç™¼è€…æ‰‹å‹•æ“ä½œã€‚æ¯ä¸€æ¬¡ `generate_content` è«‹æ±‚ï¼Œç³»çµ±æœƒè‡ªå‹•ç‚ºè¼¸å…¥å…§å®¹å»ºç«‹å¿«å–ã€‚ç•¶å¾ŒçºŒè«‹æ±‚èˆ‡æ­¤å‰å…§å®¹å®Œå…¨ä¸€è‡´æ™‚ï¼Œå°‡ç›´æ¥å‘½ä¸­å¿«å–ï¼Œè¿”å›ä¸Šä¸€æ¬¡çš„æ¨ç†çµæœï¼Œå¤§å¹…æå‡å›æ‡‰é€Ÿåº¦ä¸¦æœ‰æ©Ÿæœƒç¯€çœ token æ¶ˆè€—ã€‚


- **å¿«å–è‡ªå‹•ç”Ÿæ•ˆï¼Œç„¡éœ€æ‰‹å‹•é…ç½®ã€‚**
- å¿«å–åƒ…åœ¨å…§å®¹ã€æ¨¡å‹ã€åƒæ•¸å®Œå…¨ä¸€è‡´æ™‚ç”Ÿæ•ˆï¼›ä»»ä½•æ¬„ä½ä¸åŒéƒ½æœƒè¦–ç‚ºæ–°è«‹æ±‚ï¼Œä¸å‘½ä¸­å¿«å–ã€‚
- å¿«å–æœ‰æ•ˆæœŸï¼ˆTTLï¼‰ç”±é–‹ç™¼è€…è¨­å®šï¼Œä¹Ÿå¯ä»¥ä¸è¨­å®šã€‚å¦‚æœæœªæŒ‡å®šï¼Œé è¨­ç‚º 1 å°æ™‚ã€‚ç„¡æœ€å°æˆ–æœ€å¤§æ™‚é•·é™åˆ¶ï¼Œè²»ç”¨å–æ±ºæ–¼å¿«å– token æ•¸èˆ‡å¿«å–æ™‚é–“ã€‚
  - é›–ç„¶ Google å®˜æ–¹å° TTL ä¸è¨­ä¸Šä¸‹é™ï¼Œä½†ç”±æ–¼æˆ‘å€‘ä½œç‚ºè½‰ç™¼å¹³å°ï¼Œ**åƒ…æ”¯æ´æœ‰é™çš„ TTL é…ç½®ç¯„åœï¼Œä¸ä¿è­‰æ°¸ä¹…æœ‰æ•ˆ**ã€‚

### æ³¨æ„äº‹é …

- **ç„¡æˆæœ¬ç¯€çœä¿è­‰**ï¼šå¿«å– token çš„è¨ˆè²»ç‚ºè¼¸å…¥åŸåƒ¹çš„ 25%ï¼Œç†è«–ä¸Šè¼¸å…¥éƒ¨åˆ†å¯æœ€å¤šç¯€çœ 75% æˆæœ¬ï¼Œ[**ä½† Google å®˜æ–¹ä¸¦æœªæ‰¿è«¾å¿…ç„¶ç¯€çœ**](https://ai.google.dev/gemini-api/docs/caching?lang=python)ï¼Œå¯¦éš›å¸³å–®é‚„éœ€çµåˆå¿«å–å‘½ä¸­ç‡ã€token é¡å‹èˆ‡å„²å­˜æ™‚é•·å…±åŒè©•ä¼°ã€‚
- **å¿«å–å‘½ä¸­æ¢ä»¶**ï¼šå»ºè­°å°‡é‡è¤‡çš„ä¸Šä¸‹æ–‡æ”¾åœ¨è«‹æ±‚å‰éƒ¨ï¼Œå°‡æ˜“è®Šå…§å®¹ï¼ˆå¦‚ç”¨æˆ¶è¼¸å…¥ï¼‰ç½®æ–¼å¾Œéƒ¨ï¼Œä»¥æé«˜å¿«å–å‘½ä¸­ç‡ã€‚
- **å¿«å–å‘½ä¸­å›é¥‹**ï¼šå¦‚æœå›æ‡‰çµæœå‘½ä¸­å¿«å–ï¼Œåœ¨ `response.usage_metadata` ä¸­æœƒåŒ…å« `cache_tokens_details` æ¬„ä½ï¼Œä¸¦æœ‰ `cached_content_token_count`ï¼Œé–‹ç™¼è€…å¯ä»¥æ“šæ­¤åˆ¤æ–·æœ¬æ¬¡è«‹æ±‚æ˜¯å¦å‘½ä¸­å¿«å–ã€‚\
  ç¯„ä¾‹å›æ‡‰æ¬„ä½ï¼ˆå‘½ä¸­å¿«å–æ™‚ï¼‰ï¼š

  ```
  cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=2003)]
  cached_content_token_count=2003
  ```

**ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š**

```python
from google import genai

client = genai.Client(
    http_options={"base_url": "https://aihubmix.com/gemini"},
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
)

prompt = """
ã€Šä¸‰å›½æ¼”ä¹‰ã€‹è¯æ›°ï¼šæ‰“å¼€å­—å…¸    æ»šæ»šé•¿æ±Ÿä¸œé€æ°´ï¼ŒæµªèŠ±æ·˜å°½è‹±é›„ã€‚æ˜¯éæˆè´¥è½¬å¤´ç©ºï¼šé’å±±ä¾æ—§åœ¨ï¼Œå‡ åº¦å¤•é˜³çº¢ã€‚ç™½å‘æ¸”æ¨µæ±Ÿæ¸šä¸Šï¼Œæƒ¯çœ‹ç§‹æœˆæ˜¥é£ã€‚ä¸€å£¶æµŠé…’å–œç›¸é€¢ï¼šå¤ä»Šå¤šå°‘äº‹ï¼Œéƒ½ä»˜ç¬‘è°ˆä¸­ã€‚2 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : è¯è¯´å¤©ä¸‹å¤§åŠ¿ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ï¼šå‘¨æœ«ä¸ƒå›½åˆ†äº‰ï¼Œå¹¶å…¥äºç§¦ã€‚åŠç§¦ç­ä¹‹åï¼Œæ¥šã€æ±‰åˆ†äº‰ï¼Œåˆå¹¶å…¥äºæ±‰ã€‚æ±‰æœè‡ªé«˜ç¥–æ–©ç™½è›‡è€Œèµ·ä¹‰ï¼Œä¸€ç»Ÿå¤©ä¸‹ã€‚åæ¥å…‰æ­¦ä¸­å…´ï¼Œä¼ è‡³çŒ®å¸ï¼Œé‚åˆ†ä¸ºä¸‰å›½ã€‚æ¨å…¶è‡´ä¹±ä¹‹ç”±ï¼Œæ®†å§‹äºæ¡“ã€çµäºŒå¸ã€‚æ¡“å¸ç¦é”¢å–„ç±»ï¼Œå´‡ ä¿¡å®¦å®˜ã€‚åŠæ¡“å¸å´©ï¼Œçµå¸å³ä½ï¼Œå¤§å°†å†›çª¦æ­¦ã€å¤ªå‚…é™ˆè•ƒï¼Œå…±ç›¸è¾…ä½ã€‚æ—¶æœ‰å®¦å®˜æ›¹èŠ‚ç­‰å¼„æƒï¼Œçª¦æ­¦ã€é™ˆè•ƒè°‹è¯›ä¹‹ï¼Œä½œäº‹ä¸å¯†ï¼Œåä¸ºæ‰€å®³ã€‚ä¸­æ¶“è‡ªæ­¤æ„ˆæ¨ªã€‚3 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å»ºå®äºŒå¹´å››æœˆæœ›æ—¥ï¼Œå¸å¾¡æ¸©å¾·æ®¿ã€‚æ–¹å‡åº§ï¼Œæ®¿è§’ç‹‚é£éª¤èµ·ï¼Œåª è§ä¸€æ¡å¤§é’è›‡ï¼Œä»æ¢ä¸Šé£å°†ä¸‹æ¥ï¼ŒèŸ äºæ¤…ä¸Šã€‚å¸æƒŠå€’ï¼Œå·¦å³æ€¥æ•‘å…¥å®«ï¼Œç™¾å®˜ä¿±å¥”é¿ã€‚é¡»è‡¾ï¼Œè›‡ä¸è§äº†ã€‚å¿½ç„¶å¤§é›·å¤§é›¨ï¼ŒåŠ ä»¥å†°é›¹ï¼Œè½åˆ°åŠå¤œæ–¹æ­¢ï¼Œåå´æˆ¿å±‹æ— æ•°ã€‚å»ºå®å››å¹´äºŒæœˆï¼Œæ´›é˜³åœ°éœ‡ï¼›åˆæµ·æ°´æ³›æº¢ï¼Œæ²¿æµ·å±…æ°‘ï¼Œå°½è¢«å¤§æµªå·å…¥æµ·ä¸­ã€‚å…‰å’Œå…ƒ å¹´ï¼Œé›Œé¸¡åŒ–é›„ã€‚å…­æœˆæœ”ï¼Œé»‘æ°”åé¦€ä¸ˆï¼Œé£å…¥æ¸©å¾·æ®¿ä¸­ã€‚ç§‹ä¸ƒæœˆï¼Œæœ‰è™¹è§äºç‰å ‚ï¼›äº”åŸå±±å²¸ï¼Œå°½çš†å´©è£‚ã€‚ç§ç§ä¸ç¥¥ï¼Œéæ­¢ä¸€ç«¯ã€‚4 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å¸ä¸‹è¯é—®ç¾¤è‡£ä»¥ç¾å¼‚ä¹‹ç”±ï¼Œè®®éƒè”¡é‚•ä¸Šç–ï¼Œä»¥ä¸ºéœ“å •é¸¡åŒ–ï¼Œä¹ƒå¦‡å¯ºå¹²æ”¿ä¹‹æ‰€è‡´ï¼Œè¨€é¢‡åˆ‡ ç›´ã€‚å¸è§ˆå¥å¹æ¯ï¼Œå› èµ·æ›´è¡£ã€‚æ›¹èŠ‚åœ¨åçªƒè§†ï¼Œæ‚‰å®£å‘Šå·¦å³Â·é‚ä»¥ä»–äº‹é™·é‚•äºç½ªï¼Œæ”¾å½’ç”°é‡Œã€‚åå¼ è®©ï¼Œèµµå¿ ï¼Œå°è«ï¼Œæ®µåœ­ï¼Œæ›¹èŠ‚ï¼Œå€™è§ˆï¼Œè¹‡ç¡•ï¼Œç¨‹æ—·ï¼Œå¤æ½ï¼Œéƒ­èƒœåäººæœ‹æ¯”ä¸ºå¥¸ï¼Œå·ä¸º"åå¸¸ä¾"ã€‚å¸å°Šä¿¡å¼ è®©ï¼Œå‘¼ä¸º"é˜¿çˆ¶"ï¼Œæœæ”¿æ—¥éï¼Œä»¥è‡´å¤©ä¸‹äººå¿ƒæ€ä¹±ï¼Œç›—è´¼èœ‚èµ·ã€‚5 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : æ—¶é’œé¹¿éƒ¡æœ‰å…„å¼Ÿä¸‰äººï¼šä¸€åå¼ è§’ï¼Œä¸€åå¼ å®ï¼Œä¸€åå¼ æ¢ã€‚é‚£å¼ è§’æœ¬æ˜¯ä¸ªä¸ç¬¬ç§€æ‰ã€‚å› å…¥å±±é‡‡è¯ï¼Œé‡ä¸€è€äººï¼Œç¢§çœ¼ç«¥é¢œï¼Œæ‰‹æ‰§è—œæ–ï¼Œå”¤è§’è‡³ä¸€æ´ä¸­ï¼Œä»¥å¤©ä¹¦ä¸‰å·æˆä¹‹ï¼Œæ›°ï¼š"æ­¤åå¤ªå¹³è¦æœ¯ã€‚æ±å¾—ä¹‹ï¼Œå½“ä»£å¤©å®£åŒ–ï¼Œæ™®æ•‘ä¸–äººï¼›è‹¥èŒå¼‚å¿ƒï¼Œå¿…è·æ¶æŠ¥ã€‚"è§’æ‹œé—®å§“åã€‚è€äººæ›°ï¼š"å¾ä¹ƒå—åè€ä»™ä¹Ÿã€‚"è¨€è®«ï¼ŒåŒ–é˜µæ¸…é£è€Œå»ã€‚6 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : è§’å¾—æ­¤ä¹¦ï¼Œæ™“å¤œæ”»ä¹ ï¼Œèƒ½å‘¼é£å”¤é›¨ï¼Œå·ä¸ºå¤ªå¹³é“äººã€‚ä¸­å¹³å…ƒå¹´æ­£æœˆå†…ï¼Œç–«æ°”æµè¡Œï¼Œå¼ è§’æ•£æ–½ç¬¦æ°´ï¼Œä¸ºäººæ²»ç—…ï¼Œè‡ªç§°å¤§è´¤è‰¯å¸ˆã€‚è§’æœ‰å¾’å¼Ÿäº”ç™¾é¦€äººï¼Œäº‘æ¸¸å››æ–¹ï¼Œçš†èƒ½ä¹¦ç¬¦å¿µå’’ã€‚æ¬¡åå¾’ä¼—æ—¥å¤šï¼Œè§’ä¹ƒç«‹ä¸‰åå…­æ–¹ï¼Œâ”€å¤§æ–¹ä¸‡é¦€äººï¼Œå°æ–¹å…­ä¸ƒåƒâ”€ï¼Œå„ç«‹æ¸ å¸…ï¼Œç§°ä¸ºå°†å†›ã€‚è®¹è¨€"è‹å¤©å·²æ­»ï¼Œé»„å¤©å½“ç«‹ã€‚"åˆäº‘"å²åœ¨ç”²å­ï¼Œå¤©ä¸‹å¤§å‰ã€‚"ä»¤äººå„ä»¥ç™½åœŸï¼Œä¹¦"ç”²å­"äºŒå­—äºå®¶ä¸­å¤§é—¨ä¸Šã€‚é’ã€å¹½ã€å¾ã€å†€ã€è†ã€æ‰¬ã€å…–ã€è±«å…«å·ä¹‹äººï¼Œå®¶å®¶ä¾å¥‰å¤§è´¤è‰¯å¸ˆå¼ è§’åå­—ã€‚è§’é£å…¶å…šé©¬å…ƒä¹‰ï¼Œæš—èµé‡‘å¸›ï¼Œç»“äº¤ä¸­æ¶“å°è«ï¼Œä»¥ä¸ºå†…åº”ã€‚è§’ä¸äºŒå¼Ÿå•†è®®æ›°ï¼š"è‡³éš¾å¾—è€…ï¼Œæ°‘å¿ƒä¹Ÿã€‚ä»Šæ°‘å¿ƒå·²é¡ºï¼Œè‹¥ä¸ä¹˜åŠ¿å–å¤©ä¸‹ï¼Œè¯šä¸ºå¯æƒœã€‚"é‚ä¸€é¢ç§é€ é»„æ——ï¼Œçº¦æœŸä¸¾äº‹ï¼›ä¸€é¢ä½¿å¼Ÿå­å”å·ï¼Œé©°ä¹¦æŠ¥å°è«ã€‚å”å·ä¹ƒè¿³èµ´çœä¸­å‘Šå˜ã€‚å¸å¬å¤§å°†å†›ä½•è¿›è°ƒå…µæ“’é©¬å…ƒä¹‰ï¼Œæ–©ä¹‹ï¼›æ¬¡æ”¶å°è«ç­‰ä¸€å¹²äººä¸‹ç‹±ã€‚7 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : å¼ è§’é—»çŸ¥äº‹éœ²ï¼Œæ˜Ÿå¤œä¸¾å…µï¼Œè‡ªç§°å¤©å…¬å°†å†›ï¼Œâ”€å¼ å®ç§°åœ°å…¬å°†å†›ï¼Œå¼ æ¢ç§°äººå…¬å°†å†›â”€ã€‚ç”³è¨€äºä¼—æ›°ï¼š"ä»Šæ±‰è¿å°†ç»ˆï¼Œå¤§åœ£äººå‡ºï¼›æ±ç­‰çš†å®œé¡ºä»å¤©æ„ï¼Œä»¥æ¡¨å¤ªå¹³ã€‚"å››æ–¹ç™¾å§“ï¼Œè£¹é»„å·¾ä»å¼ è§’åè€…ï¼Œå››äº”åä¸‡ã€‚è´¼åŠ¿æµ©å¤§ï¼Œå®˜å†›æœ›é£è€Œé¡ã€‚ä½•è¿›å¥å¸ç«é€Ÿé™è¯ï¼Œä»¤å„å¤„å¤‡å¾¡ï¼Œè®¨è´¼ç«‹åŠŸï¼›ä¸€é¢é£ä¸­éƒå°†å¢æ¤ï¼Œçš‡ç”«åµ©ï¼Œæœ±éš½ï¼Œå„å¼•ç²¾å…µï¼Œåˆ†ä¸‰è·¯è®¨ä¹‹ã€‚8 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : ä¸”è¯´å¼ è§’ä¸€å†›ï¼Œå‰çŠ¯å¹½å·ç•Œåˆ†ã€‚å¹½å·å¤ªå®ˆåˆ˜ç„‰ï¼Œä¹ƒæ±Ÿå¤ç«Ÿé™µäººæ°ï¼Œæ±‰é²æ­ç‹ä¹‹åä¹Ÿï¼›å½“æ—¶é—»å¾—è´¼å…µå°†è‡³ï¼Œå¬æ ¡å°‰é‚¹é–è®¡è®®ã€‚é–æ›°ï¼š"è´¼å…µä¼—ï¼Œæˆ‘å…µå¯¡ï¼Œæ˜å…¬å®œä½œé€Ÿæ‹›å†›åº”æ•Œã€‚"åˆ˜ç„‰ç„¶å…¶è¯´ï¼Œéš å³å‡ºæ¦œæ‹›å‹Ÿä¹‰å…µã€‚æ¦œæ–‡è¡Œåˆ°æ¶¿å¿ï¼Œä¹ƒå¼•å‡ºæ¶¿å¿ä¸­ä¸€ä¸ªè‹±é›„ã€‚9 æ‰“å¼€å­—å…¸  å®´æ¡ƒå›­è±ª... : é‚£äººä¸ç”šå¥½è¯»ä¹¦ï¼›æ€§å®½å’Œï¼Œå¯¡è¨€è¯­ï¼Œå–œæ€’ä¸å½¢äºè‰²ï¼›ç´ æœ‰å¤§å¿—ï¼Œä¸“å¥½ç»“äº¤å¤©ä¸‹è±ªæ°ï¼›ç”Ÿå¾—èº«é•¿ä¸ƒå°ºäº”å¯¸ï¼Œä¸¤è€³å‚è‚©ï¼ŒåŒæ‰‹è¿‡è†ï¼Œç›®èƒ½è‡ªé¡¾å…¶è€³ï¼Œé¢å¦‚å† ç‰ï¼Œå”‡è‹¥æ¶‚è„‚ï¼›ä¸­å±±é–ç‹åˆ˜èƒœä¹‹åï¼Œæ±‰æ™¯å¸é˜ä¸‹ç„å­™ï¼›å§“åˆ˜ï¼Œåå¤‡ï¼Œå­—ç„å¾·ã€‚æ˜”åˆ˜èƒœä¹‹å­åˆ˜è´ï¼Œæ±‰æ­¦æ—¶å°æ¶¿é¹¿äº­ä¾¯ï¼Œååé…¬é‡‘å¤±ä¾¯ï¼Œå› æ­¤é—è¿™ä¸€æåœ¨æ¶¿å¿ã€‚ç„å¾·ç¥–åˆ˜é›„ï¼Œçˆ¶åˆ˜å¼˜ã€‚å¼˜æ›¾ä¸¾å­å»‰ï¼Œäº¦å°ä½œåï¼Œæ—©ä¸§ã€‚ç„å¾·å¹¼å­¤ï¼Œäº‹æ¯è‡³å­ï¼›å®¶è´«ï¼Œè´©å±¦ ç»‡å¸­ä¸ºä¸šã€‚å®¶ä½æœ¬å¿æ¥¼æ¡‘æ‘ã€‚å…¶å®¶ä¹‹ä¸œå—ï¼Œæœ‰ä¸€å¤§æ¡‘æ ‘ï¼Œé«˜äº”ä¸ˆé¦€ï¼Œé¥æœ›ä¹‹ï¼Œç«¥ç«¥å¦‚è½¦ç›–ã€‚ç›¸è€…äº‘ï¼š"æ­¤å®¶å¿…å‡ºè´µäººã€‚"10 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : ç„å¾·å¹¼æ—¶ï¼Œä¸ä¹¡ä¸­å°å„¿æˆäºæ ‘ä¸‹ï¼Œæ›°ï¼š"æˆ‘ä¸ºå¤©å­ï¼Œå½“ä¹˜æ­¤è½¦ç›–ã€‚"å”çˆ¶åˆ˜å…ƒèµ·å¥‡å…¶è¨€ï¼Œæ›°ï¼š"æ­¤å„¿éå¸¸äººä¹Ÿï¼"å› è§ç„å¾·å®¶è´«ï¼Œå¸¸èµ„ç»™ä¹‹ã€‚å¹´åäº”å²ï¼Œæ¯ä½¿æ¸¸å­¦ï¼Œå°å¸ˆäº‹éƒ‘ç„ã€å¢æ¤ï¼›ä¸å…¬å­™ç“’ç­‰ä¸ºå‹ã€‚åŠåˆ˜ç„‰å‘æ¦œæ‹›å†›æ—¶ï¼Œç„å¾·å¹´å·±äºŒåå…«å²çŸ£ã€‚å½“æ—¥è§äº†æ¦œæ–‡ï¼Œæ…¨ç„¶é•¿å¹ã€‚éšåä¸€äººå‰å£°è¨€æ›°ï¼š"å¤§ä¸ˆå¤«ä¸ä¸å›½å®¶å‡ºåŠ›ï¼Œä½•æ•…é•¿å¹ï¼Ÿ"11 æ‰“å¼€å­— å…¸ å®´æ¡ƒå›­è±ª... : ç„å¾·å›è§†å…¶äººï¼šèº«é•¿å…«å°ºï¼Œè±¹å¤´ç¯çœ¼ï¼Œç‡•é¢”è™é¡»ï¼Œå£°è‹¥å·¨é›·ï¼ŒåŠ¿å¦‚å¥”é©¬ã€‚ç„å¾·è§ä»–å½¢è²Œå¼‚å¸¸ï¼Œé—®å…¶å§“åã€‚å…¶äººæ›°ï¼š"æŸå§“å¼ ï¼Œåé£ï¼Œå­—ç¿¼å¾·ã€‚ä¸–å±…æ¶¿éƒ¡ï¼Œé¢‡æœ‰åº„ç”°ï¼Œå–é…’å± çŒªï¼Œä¸“å¥½ç»“äº¤å¤©ä¸‹è±ªæ°ã€‚é€‚æ‰è§å…¬çœ‹æ¦œè€Œå¹ï¼Œæ•…æ­¤ç›¸ é—®ã€‚"ç„å¾·æ›°ï¼š"æˆ‘æœ¬æ±‰å®¤å®—äº²ï¼Œå§“åˆ˜ï¼Œåå¤‡ã€‚ä»Šé—»é»„å·¾å€¡ä¹±ï¼Œæœ‰å¿—æ¬²ç ´è´¼å®‰æ°‘ï¼›æ¨åŠ›ä¸èƒ½ï¼Œæ•…é•¿å¹è€³ã€‚"é£æ›°ï¼š"å¾é¢‡æœ‰èµ„è´¢ï¼Œå½“æ‹›å‹Ÿä¹¡å‹‡ï¼Œä¸å…¬åŒä¸¾å¤§äº‹ï¼Œå¦‚ä½•ï¼Ÿ"ç„å¾·ç”šå–œï¼Œé‚ä¸åŒå…¥æ‘åº—ä¸­é¥®é…’ã€‚12 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : æ­£é¥®é—´ï¼Œè§ä¸€å¤§æ±‰ï¼Œæ¨è‘—ä¸€è¾†è½¦å­ï¼Œåˆ°åº—é—¨é¦–æ­‡äº†ï¼›å…¥åº—åä¸‹ï¼Œä¾¿å”¤é…’ä¿ï¼š"å¿«æ–Ÿé…’æ¥åƒï¼Œæˆ‘å¾…èµ¶å…¥åŸå»æŠ•å†›ã€‚"ç„å¾·çœ‹å…¶äººï¼šèº«é•¿ä¹å°ºï¼Œé«¯é•¿äºŒå°ºï¼šé¢å¦‚é‡æ£ï¼Œå”‡è‹¥æ¶‚è„‚ï¼›ä¸¹å‡¤çœ¼ï¼Œå§èš•çœ‰ï¼šç›¸è²Œå ‚å ‚ï¼Œå¨é£å‡›å‡›ã€‚ç„å¾·å°±é‚€ä»–åŒåï¼Œå©å…¶å§“åã€‚å…¶äººæ›°ï¼š"å¾å§“å…³ï¼Œåç¾½ï¼Œå­—å¯¿é•¿ï¼Œåæ”¹äº‘é•¿ï¼Œæ²³ä¸œè§£è‰¯äººä¹Ÿã€‚å› æœ¬å¤„åŠ¿è±ªï¼Œå€šåŠ¿å‡Œäººï¼Œè¢«å¾æ€äº†ï¼›é€ƒéš¾æ±Ÿæ¹–ï¼Œäº”å…­å¹´çŸ£ã€‚ä»Šé—»æ­¤å¤„æ‹›å†›ç ´è´¼ï¼Œç‰¹æ¥åº”å‹Ÿã€‚"ç„å¾·é‚ä»¥å·±å¿—å‘Šä¹‹ã€‚äº‘é•¿å¤§å–œã€‚åŒåˆ°å¼ é£åº„ä¸Šï¼Œå…±è®®å¤§äº‹ã€‚13 æ‰“å¼€å­—å…¸ç›¸å…³è®¨è®º å®´æ¡ƒå›­è±ª... : é£æ›°ï¼š"å¾åº„åæœ‰ä¸€æ¡ƒå›­ï¼ŒèŠ±å¼€æ­£ç››ï¼›æ˜æ—¥å½“äºå›­ä¸­ç¥­å‘Šå¤©åœ°ï¼Œæˆ‘ä¸‰äººç»“ä¸ºå…„å¼Ÿï¼ŒååŠ›åŒå¿ƒï¼Œç„¶åå¯å›¾å¤§äº‹ã€‚"ç„å¾·ã€äº‘é•¿ã€é½å£°åº”æ›°ï¼š"å¦‚æ­¤ç”šå¥½ã€‚"æ¬¡æ—¥ï¼Œäºæ¡ƒå›­ä¸­ï¼Œå¤‡ä¸‹ä¹Œç‰›ç™½é©¬ç¥­ç¤¼ç­‰é¡¹ï¼Œä¸‰äººç„šé¦™ï¼Œå†æ‹œè€Œè¯´èª“æ›°ï¼š"å¿µåˆ˜å¤‡ã€å…³ç¾½ã€å¼ é£ï¼Œè™½ç„¶å¼‚å§“ï¼Œæ—¢ç»“ä¸ºå…„å¼Ÿï¼Œåˆ™åŒå¿ƒååŠ›ï¼Œæ•‘å›°æ‰¶å±ï¼›ä¸ŠæŠ¥å›½å®¶ï¼Œä¸‹å®‰é»åº¶ï¼›ä¸æ±‚åŒå¹´åŒæœˆåŒæ—¥ç”Ÿï¼Œä½†æ„¿åŒå¹´åŒæœˆåŒæ—¥æ­»ã€‚çš‡å¤©ååœŸï¼Œå®é‰´æ­¤å¿ƒã€‚èƒŒä¹‰å¿˜æ©ï¼Œå¤©äººå…±æˆ®ã€‚"èª“æ¯•ï¼Œæ‹œç„å¾·ä¸ºå…„ï¼Œå…³ç¾½æ¬¡ä¹‹ï¼Œå¼ é£ä¸ºå¼Ÿã€‚ç¥­ç½¢å¤©åœ°ï¼Œå¤å®°ç‰›è®¾é…’ï¼Œèšä¹¡ä¸­å‹‡å£«ï¼Œå¾—ä¸‰ç™¾é¦€äººï¼Œå°±æ¡ƒå›­ä¸­ç—›é¥®ä¸€é†‰ã€‚æ¥æ—¥æ”¶æ‹¾å†›å™¨ï¼Œä½†æ¨æ— é©¬åŒ¹å¯ä¹˜ã€‚14 æ‰“å¼€å­—å…¸ å®´æ¡ƒå›­è±ª... : æ­£æ€è™‘é—´ï¼ŒäººæŠ¥"æœ‰ä¸¤ä¸ªå®¢äººï¼Œå¼•ä¸€å¤¥ä¼´å½“ï¼Œèµ¶ä¸€ç¾¤é©¬ï¼ŒæŠ•åº„ä¸Šæ¥ã€‚"ç„å¾·æ›°ï¼š"æ­¤å¤©ä½‘æˆ‘ä¹Ÿï¼"ä¸‰äººå‡ºåº„è¿æ¥ã€‚åŸæ¥äºŒå®¢ä¹ƒä¸­å±±å¤§å•†ï¼šä¸€åå¼ ä¸–å¹³ï¼Œä¸€åè‹åŒï¼Œæ¯å¹´å¾€åŒ—è´©é©¬ï¼Œè¿‘å› å¯‡å‘è€Œå›ã€‚ç„å¾·è¯·äºŒäººåˆ°åº„ï¼Œç½®é…’ç®¡å¾…ï¼Œè¯‰è¯´æ¬²è®¨è´¼å®‰æ°‘ä¹‹æ„ã€‚äºŒå®¢å¤§å–œï¼Œæ„¿å°†è‰¯é©¬äº”ååŒ¹ç›¸é€ï¼›åˆèµ é‡‘é“¶äº”ç™¾ä¸¤ï¼Œé•”é“ä¸€åƒæ–¤ï¼Œä»¥èµ„å™¨ç”¨ã€‚ç„å¾·è°¢åˆ«äºŒå®¢ï¼Œä¾¿å‘½è‰¯åŒ æ‰“é€ åŒè‚¡å‰‘ã€‚äº‘é•¿é€ é’é¾™åƒæœˆåˆ€ï¼Œåˆåå†·è‰³ é”¯ï¼Œé‡å…«åäºŒæ–¤ã€‚å¼ é£é€ ä¸ˆå…«ç‚¹é’¢çŸ›ã€‚å„ç½®å…¨èº«é“ ç”²ã€‚å…±èšä¹¡å‹‡äº”ç™¾é¦€äººï¼Œæ¥è§é‚¹é–ã€‚é‚¹é–å¼•è§å¤ªå®ˆåˆ˜ç„‰ã€‚ä¸‰äººå‚è§æ¯•ï¼Œå„é€šå§“åã€‚ç„å¾·è¯´èµ·å®—æ´¾ï¼Œåˆ˜ç„‰å¤§å–œï¼Œé‚è®¤ç„å¾·ä¸ºä¾„ã€‚
"""

def generate_content_sync():
    response = client.models.generate_content(
        model="gemini-2.5-flash-preview-05-20",
        contents=prompt + "ä¸‰åœ‹æ¼”ç¾©é€™ä¸€å›å‡ºç¾äº†å¹¾å€‹ä»»å‹™ ",
    )
    print(response.usage_metadata)  # å‘½ä¸­å¿«å–æ™‚æœƒé¡¯ç¤º cache_tokens_details å’Œ cached_content_token_count æ¬„ä½
    return response

generate_content_sync()
```

> å‘½ä¸­å¿«å–æ™‚ï¼Œ`response.usage_metadata` æœƒåŒ…å«å¦‚ä¸‹çµæ§‹ï¼š
>
> ```
> cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=2003)]
> cached_content_token_count=2003
> ```

**æ ¸å¿ƒçµè«–ï¼š**éš±å¼å¿«å–æ”¯æ´è‡ªå‹•å‘½ä¸­èˆ‡å‘½ä¸­å›é¥‹ã€‚é–‹ç™¼è€…å¯ä»¥é€šé usage_metadata åˆ¤æ–·å‘½ä¸­æƒ…æ³ã€‚æˆæœ¬ç¯€çœéä¿è­‰ï¼Œå¯¦éš›æ•ˆæœå› è«‹æ±‚çµæ§‹å’Œä½¿ç”¨å ´æ™¯è€Œç•°ã€‚

## Function calling

ä½¿ç”¨ openai ç›¸å®¹æ–¹å¼èª¿ç”¨ Gemini çš„ function calling åŠŸèƒ½æ™‚ï¼Œéœ€è¦åœ¨è«‹æ±‚é«”å…§éƒ¨å‚³å…¥`tool_choice="auto"`ï¼Œå¦å‰‡æœƒå ±éŒ¯ã€‚

<CodeGroup>

```py Python
from openai import OpenAI

# å®šç¾©æ¨¡å‹çš„ function å®£å‘Š
def schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# é…ç½® client
client = OpenAI(
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

# ç”¨ OpenAI ç›¸å®¹æ ¼å¼ç™¼é€å¸¶æœ‰ function å®£å‘Šçš„è«‹æ±‚
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
        {"role": "user", "content": "Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning."}
    ],
    tools=[{"type": "function", "function": schedule_meeting_function}],
    tool_choice="auto" ## ğŸ“ æ­¤è™•è¿½åŠ äº† Aihubmix ç›¸å®¹ï¼Œæ›´ç©©å®šçš„è«‹æ±‚æ–¹å¼
)

# æª¢æŸ¥æ˜¯å¦æœ‰ function call
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    function_call = tool_call.function
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.arguments}")
    print(response.usage)
    #  åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡å¯ä»¥èª¿ç”¨ä½ çš„ functionï¼š
    #  result = schedule_meeting(**json.loads(function_call.arguments))
else:
    print("No function call found in the response.")
    print(response.choices[0].message.content)
```

</CodeGroup>

**è¼¸å‡ºçµæœç¤ºä¾‹ï¼š**

```bash
Function to call: schedule_meeting
Arguments: {"attendees":["Bob","Alice"],"date":"2025-03-14","time":"10:00","topic":"Q3 planning"}
CompletionUsage(completion_tokens=28, prompt_tokens=111, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None)
```

## Tokens ç”¨é‡è¿½è¸ª

1. Gemini åŸç”Ÿé‡‡ç”¨ `usage_metadata` ä¾†[è¿½è¸ªä½¿ç”¨çš„ token](https://ai.google.dev/gemini-api/docs/tokens?lang=python)ï¼Œå…¶ä¸­çš„å­—æ®µå¯¹åº”å¦‚ä¸‹ï¼š

- prompt_token_count: è¼¸å…¥ token æ•¸
- candidates_token_count: è¼¸å‡º token æ•¸
- thoughts_token_count: æ¨ç†ä½¿ç”¨çš„ token æ•¸ï¼Œæ€§è³ªä¸Šä¹Ÿæ˜¯è¼¸å‡º token
- total_token_count: ç¸½ token ä½¿ç”¨é‡ï¼ˆè¼¸å…¥+è¼¸å‡ºï¼‰

2. å°æ–¼ OpenAI ç›¸å®¹æ ¼å¼ï¼Œå‰‡æ¡ç”¨ `.usage` ä¾†è¿½è¹¤ï¼Œæ¬„ä½å°æ‡‰å¦‚ä¸‹ï¼š

- usage.completion_tokens: è¼¸å…¥ token æ•¸
- usage.prompt_tokens: è¼¸å‡º token æ•¸ï¼ˆåŒ…å«æ¨ç†ä½¿ç”¨çš„ token æ•¸ï¼‰
- usage.total_tokens: ç¸½ token ä½¿ç”¨é‡

**ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹:**

<CodeGroup>

```py Gemini åŸç”Ÿ
from google import genai
from google.genai import types
import time

def generate():
    client = genai.Client(
        api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
        http_options={"base_url": "https://aihubmix.com/gemini"},
    )

    model = "gemini-2.5-pro-preview-03-25"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""é‡‘èé ˜åŸŸçš„ã€Œ72 æ³•å‰‡ã€æ˜¯å¦‚ä½•æ¨å°çš„ï¼Ÿ"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    final_usage_metadata = None
    
    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")
        if chunk.usage_metadata:
            final_usage_metadata = chunk.usage_metadata
    
    # åœ¨æ‰€æœ‰ chunk è™•ç†å®Œå¾Œï¼Œæ‰“å°å®Œæ•´çš„ token ä½¿ç”¨æƒ…æ³
    if final_usage_metadata:
        print(f"\nUsage: {final_usage_metadata}")

if __name__ == "__main__":
    generate()
```


```py OpenAI ç›¸å®¹
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17",
    reasoning_effort="low", #"low", "medium", and "high", which behind the scenes we map to 1K, 8K, and 24K thinking token budgets. If you want to disable thinking, you can set the reasoning effort to "none".
    messages=[
        {
            "role": "user",
            "content": "é‡‘èé ˜åŸŸçš„ã€Œ72 æ³•å‰‡ã€æ˜¯å¦‚ä½•æ¨å°çš„ï¼Ÿ"
        }
    ],
    stream=True
)

#print(completion.choices[0].message.content)

for chunk in completion:
    print(chunk.choices[0].delta)
    # åªåœ¨æœ€å¾Œä¸€å€‹ chunkï¼ˆåŒ…å«å®Œæ•´ usage è³‡æ–™ï¼‰æ™‚æ‰“å° usage è³‡è¨Š
    if chunk.usage and chunk.usage.completion_tokens > 0:
        print(f"è¼¸å‡º tokens: {chunk.usage.completion_tokens}")
        print(f"è¼¸å…¥ tokens: {chunk.usage.prompt_tokens}")
        print(f"ç¸½ tokens: {chunk.usage.total_tokens}")
```

</CodeGroup>