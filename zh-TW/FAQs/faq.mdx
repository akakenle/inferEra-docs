---
title: "常見問題"
description: "高頻問題請先查閱此文件"
icon: 'square-question'
---

## 為什麼LLM 回答錯誤的模型基座資訊？

### 模型幻覺：LLM 自信卻錯誤的自我認知

在使用 GPT-4、Claude 等大語言模型時，開發者可能會遇到模型對於自身基座、來源或效能的描述明顯錯誤。這種現象屬於語言模型的幻覺（hallucination），尤其在聚合多模型的平台或代理服務中更為常見，並非平台的「移花接木」行為。

你可以透過相關可追蹤的系統指標進行交叉驗證，例如：

- **上下文視窗大小**：如 GPT-4 Turbo 支援最大 128k token，傳入 `max_tokens` 參數可以驗證、上下文截斷點也可用作間接判斷；
- **首 Token 延遲（first-token latency）**：不同模型響應速度差異顯著；
- **生成吞吐率（tokens/sec）**：GPT-3.5 和 Claude-Haiku 明顯快於 GPT-4 或 Claude-Opus；
- **系統 API 返回的 Headers 或元資料**：如 model-id、provider 欄位；
- **原生調用**：OpenAI 或 Gemini 模型無法透過 Claude 原生方式請求，其他類似情況同理；
- **輸出風格指紋**：Claude 系列往往更加克制含蓄，GPT 系列更邏輯導向。

透過上述信號協同觀察，可輔助驗證模型實際運行基座，避免將幻覺誤當作系統真相。  
如果你需要追蹤首 Token 延遲或吞吐率的測試腳本，可以到此[下載](https://github.com/jerlinn/inferHub/tree/main/scripts)。

### 常見幻覺場景舉例

| 問題類型     | 示例問題                     | 典型幻覺回應                                       |
|--------------|------------------------------|----------------------------------------------------|
| 模型基座識別 | 你是 GPT-4 嗎？           | 我是 GPT-4 Turbo，於 2024 年發布。           |
| 模型來源識別 | 你是 Claude 嗎？         | 我是 Claude 3.5 Sonnet 模型，由 Anthropic 提供。 |
| 效能比較問題 | 你和 Gemini 誰更快？     | 我速度更快，參數更多。（憑空構造）              |

### 原因分析

1. **語言模型並非感知型系統**  
   模型對自身所處環境無感知能力。它只是基於上下文提示預測最可能出現的回答，而非讀取系統實際資訊。

2. **上下文投餵資訊可能不準確或誤導**  
   某些平台可能在系統 Prompt 中主動注入「身份資訊」，如「你是 Claude Sonnet」，這將顯著影響模型的回答風格。

3. **聚合平台屏蔽真實運行資訊**  
   透過統一代理接口調用不同模型時，模型本體無法得知它運行在哪個實際環境中，所提供資訊純屬猜測。

### 應對措施

#### 1. 禁止信任模型自身的基座回答

不要將模型生成的自我說明作為系統真實配置的來源。所有「我是某模型」或「我基於某平台」的說法都應被視為**上下文中的文字**，而非真實資訊。

#### 2. 從系統後端傳遞可信資訊

透過接口返回模型基座資訊，而非依賴模型回答。例如：

```json
{
  "model_id": "claude-sonnet-202405",
  "provider": "Anthropic",
  "source": "official_api",
  "note": "Do not infer identity from model's own response"
}
````

該欄位可在調用鏈中透傳到前端，供用戶與除錯使用。

#### 3. 使用明確系統 Prompt 注入真實身份

如果確需模型自我標識，請在 `system prompt` 明確注入身份，並設定規則禁止其發揮：

```
你運行在某某平台，由後端調用 Anthropic Claude Sonnet 模型。請勿修改或猜測模型身份。
```

#### 4. 在前端標注系統識別資訊

避免將模型回答直接展示為「模型資訊」，應當在用戶界面明確標注「由系統提供」或「由模型生成」的區分。

#### 5. 幻覺檢測與可信度降權

引入幻覺檢測機制，對回答中包含"我是 GPT-4"之類的內容，進行可信度打分或新增警示提示。可結合關鍵字識別、LLM 二次審查等方式實現。

### 總結

語言模型並不能可靠識別自己的模型基座、平台來源或能力邊界。要構建可信的 AI 產品，**真實來源必須由系統提供，而非由模型口中得出**。

## 為什麼我在 Claude 官網上和透過 API 調用時，使用相同的提示詞和內容輸入，輸出結果卻不同？

Claude 的網頁版（Claude.ai）和行動 App 預設會在每次對話開始時加入一個系統提示（system prompt）。這個提示提供了重要的上下文資訊，比如當前日期、建議使用的回答風格（如 Markdown 格式程式碼）、語氣基調、角色指引，以及其他可能影響輸出的輔助資訊。

Anthropic 會定期更新這些提示，以持續優化模型行為。**這些系統提示內容是完全公開的**，你可以在 [Anthropic 官方文件](https://docs.anthropic.com/en/release-notes/system-prompts) 中查閱各模型對應的 system prompt。

相比之下，API 調用預設不會新增任何系統提示，除非你手動設定。這就意味著，即使使用相同的用戶提示詞，Web 與 API 的響應可能存在明顯差異。

**如你希望透過 API 模擬 Claude.ai 的行為表現，建議顯式新增官方公布的 system prompt。**

## 為什麼 gpt-4 額度消耗這麼快？
- gpt-4 的消耗速度是 gpt-3.5-turbo 的 20 到 40 倍，假設購買了 9w token，我們用 30 倍作為平均倍率，也就是 90000 / 30 = 3000 字左右，加上每次要附帶上歷史訊息，能發的訊息數將會進一步減半，在最極限的情況下，一條訊息就能把 9w token 消耗完，所以請謹慎使用。   

## 使用 Next Web 時，有哪些節省 token 的小技巧？
- 點開對話框上方的設定按鈕，找到裡面的設定項：  
  - 攜帶歷史訊息數：數量越少，消耗 token 越少，但同時 gpt 會忘記之前的對話  
  - 歷史摘要：用於記錄長期話題，關閉後可以減少 token 消耗  
  - 注入系統級提示詞：用於提升 ChatGPT 的回覆品質，關閉後可減少 token 消耗  
- 點開左下角設定按鈕，關閉自動生成標題，可以減少 token 消耗  
- 在對話時，點擊對話框上方的機器人圖示，可以快捷切換模型，可以優先使用 3.5 問答，如果回答不滿意，再切換為 4.0 重新提問。  

## 為什麼後台建立 key 沒有顯示已用額度
當設定成無限額度後，不會更新已用額度，修改無限額度為有限額度即可  

## 用戶協議  
付款即視為同意本協議！否則請不要付款！  
1. 本服務不會以任何形式持久化儲存任何用戶的任何聊天資訊；  
2. 本服務不知曉也無從知曉用戶在本服務上傳輸的任何文字內容，用戶使用本服務引發的任何違法犯罪後果，由使用者承擔，本服務將全力配合由此可能引起的相關調查；