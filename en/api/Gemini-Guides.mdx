---
title: "Gemini Guides"
description: "A comprehensive guide to Gemini API calls on our platform."
icon: 'google'
---

## Native Forwarding for Gemini Models

For the Gemini series, we provide **two** invocation methods: native API calls and OpenAI-compatible calls.  
Before you start, make sure to install or update the native dependency by running either `pip install google-genai` or `pip install -U google-genai`.

For native forwarding, you mainly need to inject your **AiHubMix API key** and **request URL** into the internal client setup.  
‚ö°Ô∏è Note: the URL format differs from the conventional `base_url` usage. Please refer to the example below:

```python
client = genai.Client(
    api_key="sk-***",  # üîë Replace with the key you generated from AiHubMix
    http_options={"base_url": "https://aihubmix.com"},
)
```

### About Gemini 2.5 Inference Models
1. The entire 2.5 series consists of **inference models**.
2. **2.5 Flash** is a hybrid model, similar to Claude Sonnet 3.7. You can fine-tune its reasoning behavior by adjusting the `thinking_budget` parameter for optimal control.
3. **2.5 Pro** is a pure inference model. Thinking cannot be disabled, and `thinking_budget` should not be explicitly set.

**Python usage examples:**  
<CodeGroup>
```py StreamFalse
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # üîë Replace it by your AiHubMix Key
        http_options={"base_url": "https://aihubmix.com"},
    )


    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""How do I know I'm not wasting time?"""),
            ],
        ),
    ]

    print(client.models.generate_content(
        model=model,
        contents=contents,
    ))

if __name__ == "__main__":
    generate()
```

```py 2.0 Series-Stream
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # üîë Replace it by your AiHubMix Key
        http_options={"base_url": "https://aihubmix.com"},
    )

    model = "gemini-2.0-flash"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""How do I know I'm not wasting time?"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()

```

```py 2.5 Flash-Stream
from google import genai
from google.genai import types

def generate():
    client = genai.Client(
        api_key="sk-***", # üîë Replace it by your AiHubMix Key
        http_options={"base_url": "https://aihubmix.com"},
    )


    model = "gemini-2.5-flash-preview-04-17" #gemini-2.5-pro-preview-03-25„ÄÅgemini-2.5-flash-preview-04-17
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""How do I know I'm not wasting time?"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        thinking_config = types.ThinkingConfig(
            thinking_budget=2048, #range: 0-24576„ÄÇ1024 as defaultÔºå16000 for best performance
        ),
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()
```

```py 2.5 Pro-Stream
from google import genai
from google.genai import types
import os

def generate():
    client = genai.Client(
        api_key="sk-***", # üîë Replace it by your AiHubMix Key
        http_options={"base_url": "https://aihubmix.com"},
    )

    model = "gemini-2.5-pro-preview-03-25"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text="""How do I know I'm not wasting time?"""),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for chunk in client.models.generate_content_stream(
        model=model,
        contents=contents,
        config=generate_content_config,
    ):
        print(chunk.text, end="")

if __name__ == "__main__":
    generate()

```
</CodeGroup>

## Gemini 2.5 Flash: Quick Task Support

For rapid task execution, you can use the following setup to **disable thinking** in the Flash model.  
Example for OpenAI-compatible invocation:

<CodeGroup>
```py Python
from openai import OpenAI

client = OpenAI(
    api_key="AIHUBMIX_API_KEY", # Replace with the key you generated in AiHubMix
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="gemini-2.5-flash-preview-04-17-nothink",
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ]
)

print(completion.choices[0].message.content)
```

```shell Curl
curl -X POST https://aihubmix.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-***" \
  -d '{
    "model": "gemini-2.5-flash-preview-04-17-nothink",
    "messages": [
      {
        "role": "user",
        "content": "Explain the Occam'\''s Razor concept and provide an everyday example of it."
      }
    ]
  }'
```
</CodeGroup>

For complex tasks, simply set the model id to the default `gemini-2.5-flash-preview-04-17` to enable thinking.

<Info>
Gemini 2.5 Flash uses the `budget` parameter to control the depth of thinking, ranging from 0 to 24K. The default budget is 1024, and the optimal marginal effect is 16K.
We will use the native Gemini support to allow developers to accurately control the budget.
</Info>

## Gemini Function calling

By using the openai compatible way to call Gemini's function calling, you need to pass in `tool_choice="auto"` in the request body, otherwise it will report an error.

<CodeGroup>
```py Python
from openai import OpenAI

# Define the function declaration for the model
schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# Configure the client
client = OpenAI(
    api_key="AIHUBMIX_API_KEY", # Replace with the key you generated in AiHubMix
    base_url="https://aihubmix.com/v1",
)

# Send request with function declarations using OpenAI compatible format
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
        {"role": "user", "content": "Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning."}
    ],
    tools=[{"type": "function", "function": schedule_meeting_function}],
    tool_choice="auto" ## üìç Added Aihubmix compatibility, more stable request method
)

# Check for a function call
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    function_call = tool_call.function
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.arguments}")
    print(response.usage)
    #  In a real app, you would call your function here:
    #  result = schedule_meeting(**json.loads(function_call.arguments))
else:
    print("No function call found in the response.")
    print(response.choices[0].message.content)
```
</CodeGroup>

**Output Example:**
```bash
Function to call: schedule_meeting
Arguments: {"attendees":["Bob","Alice"],"date":"2025-03-14","time":"10:00","topic":"Q3 planning"}
CompletionUsage(completion_tokens=28, prompt_tokens=111, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None)
```