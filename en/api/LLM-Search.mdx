---
title: "LLM Search"
description: " "
icon: 'globe'
---

## 1Ô∏è‚É£ Real-time Web Search: Breaking LLM Time Limitations for More Accurate and Reliable Outputs

We've enhanced OpenAI and Gemini series models with the ability to access the latest information from the web, helping you:  
- ‚úÖ **Access Latest Information**: Get real-time updates on current events, latest research, or live data
- ‚úÖ **Eliminate Knowledge Gaps**: Overcome the time limitations of LLM training data by accessing post-training information
- ‚úÖ **Reduce Hallucinations**: Provide fact-based answers through real-time web searches, significantly reducing AI confabulations
- ‚úÖ **Improve Decision Quality**: Make more confident decisions based on analysis and recommendations grounded in current facts

**Supported Models:**
Currently supporting OpenAI and Gemini model series with two integration methods:

**1. Models with Native Search Capabilities**
**Gemini Series** (Ground with Google search):
- gemini-2.0-pro-exp-02-05-search
- gemini-2.0-flash-exp-search
- gemini-2.0-flash-search

**OpenAI Series** (Bing search):
- gpt-4o-search-preview
- gpt-4o-mini-search-preview

**2. Parameter-Based Support**
Simply add the parameter `web_search_options={}` to enable web connectivity for all Gemini and OpenAI models. 
The search fee for Gemini models is $3.5 per thousand searches.

## Usage Guide

Before using, run `pip install -U openai` to upgrade the openai package.

**Example:**
<CodeGroup>
```py Python
from openai import OpenAI

client = OpenAI(
    api_key="sk-***",
    base_url="https://aihubmix.com/v1"
)

chat_completion = client.chat.completions.create(
    model="gemini-2.0-flash-exp",
    # üåê Enable search
    web_search_options={},
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Search for information about the AIhubmix LLM API platform, provide a brief introduction, and include relevant links."
                }
            ]
        }
    ]
)

print(chat_completion.choices[0].message.content)
```

```ts Typescript 
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: 'sk-***',
    baseURL: 'https://aihubmix.com/v1'
});

async function main() {
    const chatCompletion = await client.chat.completions.create({
        model: 'gemini-2.0-flash-exp',
        // üåê Enable search
        web_search_options: {},
        messages: [
            {
                role: 'user',
                content: [
                    {
                        type: 'text',
                        text: 'Search for information about the AIhubmix LLM API platform, provide a brief introduction, and include relevant links.'
                    }
                ]
            }
        ]
    });

    console.log(chatCompletion.choices[0].message.content);
}

main().catch(console.error);
```

```shell Curl
curl "https://aihubmix.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer xxx" \
    -d '{
    "model": "gemini-2.0-flash-exp",
    "web_search_options": {},
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Provide information about Van Gogh on the Google Arts & Culture website, with a brief introduction and relevant links."
                }
            ]
        }
    ],
    "stream": false
}'
```

</CodeGroup>

## 2Ô∏è‚É£ Smart Surfing: Allowing AI to Explore the Internet Freely

By appending `:surfing` to the model id, any large language model can be equipped with search capabilities.
- Simply append the suffix, no complex integration is required
- This method will default to forwarding the user's request to a search engine, and the LLM will reference the search results for response

<Tip>
The model id can be copied from the [model gallery](https://aihubmix.com/models).
</Tip>

**Example:**
<CodeGroup>
```py Python
import requests
import json
import os

try:
    response = requests.post(
        url="https://aihubmix.com/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {os.environ.get('AIHUBMIX_API_KEY')}",
            "Content-Type": "application/json",
        },
        data=json.dumps({
            "model": "gpt-4o-mini:surfing", # Append :surfing to the model id to support searching
            "messages": [
                {
                    "role": "user",
                    "content": "Search the last fact about ChatGPT memory feature, return with the URL"
                }
            ]
        })
    )

    result = response.json()
    print("API response:", json.dumps(result, ensure_ascii=False, indent=2))

except requests.exceptions.RequestException as e:
    print(f"Request error: {e}")
except json.JSONDecodeError as e:
    print(f"JSON decode error: {e}")
except Exception as e:
    print(f"Other error: {e}")
```
</CodeGroup>

**API Response Example:**
```json
{
  "id": "chatcmpl-BLN21dGcrv8MrbeHfForjY4bYZHBF",
  "model": "gpt-4o-mini-2024-07-18",
  "object": "chat.completion",
  "created": 1744433121,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The latest update about ChatGPT's memory feature indicates that it now has expanded memory capabilities that allow it to recall information in two ways: through ‚Äúsaved memories‚Äù that users manually ask it to remember, and ‚Äúreference chat history‚Äù for improving future interactions. For more details, you can visit the URL: [The Verge Article](https://www.theverge.com/news/646968/openai-chatgpt-long-term-memory-upgrade)."
      },
      "finish_reason": "stop"
    }
  ],
  "system_fingerprint": "fp_ded0d14823",
  "usage": {
    "prompt_tokens": 604,
    "completion_tokens": 89,
    "total_tokens": 693,
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  }
}
```