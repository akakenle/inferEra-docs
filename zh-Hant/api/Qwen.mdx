---
title: "é˜¿é‡Œé€šç¾©ç³»åˆ—"
icon: "a"
---

## Qwen 3 ç³»åˆ—

Qwen3 ç³»åˆ—æ˜¯é˜¿é‡Œæ¨å‡ºçš„æ–°ä¸€ä»£é–‹æºå¤§æ¨¡å‹ï¼Œèƒ½åŠ›å¤§å¹…èºå‡ï¼šåœ¨ä»£ç¢¼ç†è§£ã€æ•¸å­¸æ¨ç†ã€å¤šèªè¨€è¡¨é”ã€è¤‡é›œæ¨æ–·ä»»å‹™ä¸Šï¼Œæ¯”è‚©ç”šè‡³è¶…è¶Šäº†ç›®å‰å¸‚é¢ä¸Šçš„é ‚ç´šæ¨¡å‹ï¼ˆå¦‚ o1ã€DeepSeek-R1ï¼‰ã€‚**å®ƒçš„æ ¸å¿ƒçªç ´åœ¨äºå¼•å…¥äº†ã€Œæ€è€ƒæ¨¡å¼ã€èˆ‡ã€Œéæ€è€ƒæ¨¡å¼ã€åˆ‡æ›æ©Ÿåˆ¶ï¼Œè®“æ¨¡å‹åœ¨é¢å°ä¸åŒé›£åº¦ä»»å‹™æ™‚ï¼Œè‡ªä¸»èª¿ç¯€æ¨ç†æ·±åº¦ï¼Œå¯¦ç¾äº†é€Ÿåº¦èˆ‡ç²¾åº¦çš„é›™å„ªå¹³è¡¡ã€‚** æ——è‰¦ç‰ˆ Qwen3-235B æ¡ç”¨ç¨€ç–æ¿€æ´»ï¼Œåƒ…ç”¨ 22B åƒæ•¸æ¨ç†ï¼Œå…¼é¡§æˆæœ¬å’Œå“è¶Šèƒ½åŠ›ã€‚å…¨ç³»æ¨¡å‹å…¨é¢é–‹æºï¼Œæ¶µè“‹å¾è¼•é‡åˆ°è¶…å¤§è¦æ¨¡éœ€æ±‚ã€‚

**1. åŸºç¤ç”¨æ³•ï¼š** ç”¨ OpenAI å…¼å®¹æ ¼å¼è½‰ç™¼ã€‚  
**2. å·¥å…·èª¿ç”¨ï¼š** å¸¸è¦ Tools èª¿ç”¨æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼ï¼ˆé©ç”¨æ–¼ V2.5ã€V3ï¼‰ï¼Œè€Œ MCP Tools ä¾è³´ `qwen-agent`ï¼Œéœ€è¦å…ˆé‹è¡ŒæŒ‡ä»¤å®‰è£ä¾è³´ï¼š`pip install -U qwen-agent mcp`ã€‚
æ›´å¤šç´°ç¯€å¯ä»¥åƒè€ƒ[é˜¿é‡Œå®˜æ–¹æ–‡ä»¶](https://huggingface.co/Qwen/Qwen3-235B-A22B)

<CodeGroup>

```py åŸºç¤ç”¨æ³•
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="Qwen/Qwen3-30B-A3B",
    messages=[
        {
            "role": "user",
            "content": "Explain the Occam's Razor concept and provide everyday examples of it"
        }
    ],
    stream=True
)

# æŸäº› chunk ç‰©ä»¶å¯èƒ½æ²’æœ‰ choices å±¬æ€§æˆ– choices æ˜¯ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œè™•ç†æ–¹æ³•ï¼š
for chunk in completion:
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```

```py Tools
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

# å®šç¾©å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "ç²å–æŒ‡å®šä½ç½®çš„ç•¶å‰å¤©æ°£",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "åŸå¸‚åç¨±ï¼Œå¦‚åŒ—äº¬ã€ä¸Šæµ·ç­‰"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æº«åº¦å–®ä½"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# å‰µå»ºèŠå¤©å®Œæˆè«‹æ±‚ï¼ŒåŒ…å«å·¥å…·å®šç¾©
completion = client.chat.completions.create(
    model="Qwen/Qwen3-30B-A3B", #2.5 å’Œ 3 éƒ½æ”¯æŒï¼ŒQwQ ä¸æ”¯æŒ
    messages=[
        {
            "role": "user",
            "content": "åŒ—äº¬ä»Šå¤©çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"
        }
    ],
    tools=tools,
    tool_choice="auto",  # è®“æ¨¡å‹è‡ªè¡Œæ±ºå®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    stream=True
)

# ç”¨æ–¼æ”¶é›†å·¥å…·èª¿ç”¨ä¿¡æ¯çš„å­—å…¸
tool_calls = {}

# è™•ç†æµå¼éŸ¿æ‡‰
for chunk in completion:
    if not hasattr(chunk.choices, '__len__') or len(chunk.choices) == 0:
        continue
        
    delta = chunk.choices[0].delta
    
    # è™•ç†æ–‡æœ¬å…§å®¹
    if hasattr(delta, 'content') and delta.content:
        print(delta.content, end="")
    
    # è™•ç†å·¥å…·èª¿ç”¨
    if hasattr(delta, 'tool_calls') and delta.tool_calls:
        for tool_call in delta.tool_calls:
            if not hasattr(tool_call, 'index'):
                continue
                
            idx = tool_call.index
            if idx not in tool_calls:
                tool_calls[idx] = {"name": "", "arguments": ""}
                
            if hasattr(tool_call, 'function'):
                if hasattr(tool_call.function, 'name') and tool_call.function.name:
                    tool_calls[idx]["name"] = tool_call.function.name
                if hasattr(tool_call.function, 'arguments') and tool_call.function.arguments:
                    tool_calls[idx]["arguments"] += tool_call.function.arguments

# å®Œæˆå¾Œï¼Œæ‰“å°æ”¶é›†åˆ°çš„å·¥å…·èª¿ç”¨ä¿¡æ¯
for idx, info in tool_calls.items():
    if info["name"]:
        print(f"\nå·¥å…·èª¿ç”¨ï¼š{info['name']}")
    if info["arguments"]:
        print(f"åƒæ•¸ï¼š{info['arguments']}")

```

```py MCP Tools
from qwen_agent.agents import Assistant
import os

# Define LLM
llm_cfg = {
    'model': 'Qwen/Qwen3-30B-A3B',

    # Use a custom endpoint compatible with OpenAI API:
    'model_server': 'https://aihubmix.com/v1',
    'api_key': os.getenv('AIHUBMIX_API_KEY'),

    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define Tools
tools = [
    {'mcpServers': {  # You can specify the MCP configuration file
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            "fetch": {
                "command": "uvx",
                "args": ["mcp-server-fetch"]
            }
        }
    },
  'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)
```

</CodeGroup>

## QvQã€Qwen 2.5 å’Œ QwQ ç³»åˆ—

ç”¨ OpenAI çš„å…¼å®¹æ ¼å¼è½‰ç™¼å³å¯ï¼Œå€åˆ¥åœ¨æ–¼æµå¼èª¿ç”¨çš„æå–ï¼Œéœ€è¦å‰”é™¤ç‚ºç©ºçš„ `chunk.choices[0].delta.content`ï¼Œåƒè€ƒå¦‚ä¸‹ã€‚

**1. QvQã€Qwen 2.5 VLï¼š** åœ–ç‰‡èªè­˜  
**2. QwQï¼š** æ–‡æœ¬ä»»å‹™  

<Info>
  `Qwen/QVQ-72B-Preview` æ˜¯åŸºäº `Qwen2-VL-72B` æ§‹å»ºçš„å¼€æºå¤šæ¨¡æ…‹æ¨ç†æ¨¡å‹ï¼Œä¸“æ³¨äºè§†è§‰æ¨ç†å’Œè·¨æ¨¡æ…‹ä»»åŠ¡ã€‚
</Info>

<CodeGroup>

```py Qwen 2.5 VL
from openai import OpenAI
import base64
import os

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

image_path = "yourpath/file.png"

# è®€å–ä¸¦ç·¨ç¢¼åœ–ç‰‡
def encode_image(image_path):
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"åœ–ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{image_path}")
    
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# ç²å–åœ–ç‰‡çš„ base64 ç·¨ç¢¼
base64_image = encode_image(image_path)

# å‰µå»ºåŒ…å«æ–‡æœ¬å’Œåœ–åƒçš„æ¶ˆæ¯
completion = client.chat.completions.create(
    model="qwen2.5-vl-72b-instruct", #qwen2.5-vl-72b-instruct æˆ– Qwen/QVQ-72B-Preview
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è«‹è©³ç´°æè¿°é€™å¼µåœ–ç‰‡ï¼ŒåŒ…æ‹¬åœ–ç‰‡ä¸­çš„å…§å®¹ã€é¢¨æ ¼å’Œå¯èƒ½çš„å«ç¾©ã€‚"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{base64_image}"
                    }
                }
            ]
        }
    ],
    stream=True
)

for chunk in completion:
    # å®‰å…¨åœ°æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```


```py QwQ
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # ğŸ”‘ æ›æˆä½ åœ¨ AiHubMix ç”Ÿæˆçš„å¯†é‘°
    base_url="https://aihubmix.com/v1",
)

completion = client.chat.completions.create(
    model="Qwen/QwQ-32B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æ”¯é…å®‡å®™çš„å…ƒè¦å‰‡æ˜¯ä»€ä¹ˆï¼Ÿ"}
            ]
        }
    ],
    stream=True
)

for chunk in completion:
    if hasattr(chunk.choices, '__len__') and len(chunk.choices) > 0:
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
```

</CodeGroup>