---
title: "よくある質問"
description: "頻繁に発生する問題については、まずこの文書をご確認ください"
icon: 'square-question'
---

## なぜLLMが間違ったモデル基盤情報を回答するのですか？

### モデルの幻覚：LLMの自信に満ちた誤った自己認識

GPT-4、Claude などの大規模言語モデルを使用する際、開発者はモデルが自身の基盤、ソース、性能について明らかに間違った説明をするという現象に遭遇する可能性があります。この現象は言語モデルの幻覚（hallucination）に属し、特に複数モデルを統合するプラットフォームやプロキシサービスでより一般的であり、プラットフォームの「すり替え」行為ではありません。

以下のような追跡可能なシステム指標を通じてクロス検証を行うことができます：

- **コンテキストウィンドウサイズ**：GPT-4 Turbo は最大 128k token をサポートしており、`max_tokens` パラメータを渡すことで検証でき、コンテキストの切り捨てポイントも間接的な判断材料として使用できます；
- **初回Token遅延（first-token latency）**：異なるモデルの応答速度に顕著な差があります；
- **生成スループット（tokens/sec）**：GPT-3.5 と Claude-Haiku は GPT-4 や Claude-Opus よりも明らかに高速です；
- **システムAPIが返すHeaders またはメタデータ**：model-id、provider フィールドなど；
- **ネイティブ呼び出し**：OpenAI や Gemini モデルは Claude のネイティブ方式では要求できません。その他の類似状況も同様です；
- **出力スタイルの指紋**：Claude シリーズは控えめで含蓄的、GPT シリーズはより論理指向です。

上記のシグナルを協調的に観察することで、モデルの実際の動作基盤を検証し、幻覚をシステムの真実と誤認することを避けることができます。  
初回Token遅延やスループットのテストスクリプトが必要な場合は、こちらから[ダウンロード](https://github.com/jerlinn/inferHub/tree/main/scripts)してください。

### 一般的な幻覚シナリオの例

| 問題タイプ     | 例の質問                     | 典型的な幻覚応答                                       |
|--------------|------------------------------|----------------------------------------------------|
| モデル基盤識別 | あなたは GPT-4 ですか？           | 私は GPT-4 Turbo で、2024年にリリースされました。           |
| モデルソース識別 | あなたは Claude ですか？         | 私は Claude 3.5 Sonnet モデルで、Anthropic によって提供されています。 |
| 性能比較問題 | あなたと Gemini のどちらが速いですか？     | 私の方が速く、パラメータが多いです。（根拠のない構成）              |

### 原因分析

1. **言語モデルは感知型システムではない**  
   モデルは自身の置かれた環境を感知する能力がありません。コンテキストプロンプトに基づいて最も可能性の高い回答を予測するだけで、システムの実際の情報を読み取るわけではありません。

2. **コンテキストに供給される情報が不正確または誤解を招く可能性がある**  
   一部のプラットフォームでは、システムPromptに「身元情報」を積極的に注入する場合があります。例えば「あなたは Claude Sonnet です」などで、これはモデルの回答スタイルに大きく影響します。

3. **統合プラットフォームが実際の動作情報をマスクする**  
   統一プロキシインターフェースを通じて異なるモデルを呼び出す場合、モデル本体は自分がどの実際の環境で動作しているかを知ることができず、提供される情報は純粋に推測です。

### 対策

#### 1. モデル自身の基盤回答を信頼することを禁止

モデルが生成した自己説明をシステムの真の構成のソースとして使用しないでください。すべての「私は某モデルです」または「私は某プラットフォームに基づいています」という発言は**コンテキスト内のテキスト**として扱い、真の情報ではありません。

#### 2. システムバックエンドから信頼できる情報を渡す

インターフェースを通じてモデル基盤情報を返し、モデルの回答に依存しないでください。例：

```json
{
  "model_id": "claude-sonnet-202405",
  "provider": "Anthropic",
  "source": "official_api",
  "note": "Do not infer identity from model's own response"
}
```

このフィールドは呼び出しチェーンでフロントエンドに透過的に渡され、ユーザーとデバッグに使用されます。

#### 3. 明確なシステムPromptを使用して真の身元を注入

モデルの自己識別が本当に必要な場合は、`system prompt` で身元を明確に注入し、推測を禁止するルールを設定してください：

```
あなたは某プラットフォームで動作しており、バックエンドから Anthropic Claude Sonnet モデルを呼び出しています。モデルの身元を変更または推測しないでください。
```

#### 4. フロントエンドでシステム識別情報をマーク

モデルの回答を直接「モデル情報」として表示せず、ユーザーインターフェースで「システム提供」または「モデル生成」の区別を明確にマークする必要があります。

#### 5. 幻覚検出と信頼度の重み減少

幻覚検出メカニズムを導入し、「私は GPT-4 です」などの内容を含む回答に対して信頼度スコアリングを行ったり、警告プロンプトを追加したりします。キーワード識別、LLM 二次審査などの方法を組み合わせて実現できます。

### まとめ

言語モデルは自分のモデル基盤、プラットフォームソース、能力の境界を確実に識別することはできません。信頼できる AI 製品を構築するには、**真のソースはシステムによって提供されなければならず、モデルの発言から得るものではありません**。

## Claude の公式ウェブサイトと API 呼び出しで、同じプロンプトとコンテンツ入力を使用しているのに、出力結果が異なるのはなぜですか？

Claude のウェブ版（Claude.ai）とモバイルアプリでは、デフォルトで各会話の開始時にシステムプロンプト（system prompt）が追加されます。このプロンプトは、現在の日付、推奨される回答スタイル（Markdown 形式のコードなど）、トーン、役割ガイダンス、その他出力に影響を与える可能性のある補助情報など、重要なコンテキスト情報を提供します。

Anthropic はこれらのプロンプトを定期的に更新して、モデルの動作を継続的に最適化しています。**これらのシステムプロンプトの内容は完全に公開されており**、[Anthropic 公式ドキュメント](https://docs.anthropic.com/en/release-notes/system-prompts) で各モデルに対応する system prompt を確認できます。

一方、API 呼び出しでは、手動で設定しない限り、デフォルトではシステムプロンプトは追加されません。これは、同じユーザープロンプトを使用しても、Web と API の応答に明らかな違いが生じる可能性があることを意味します。

**API を通じて Claude.ai の動作を模倣したい場合は、公式に公開されている system prompt を明示的に追加することをお勧めします。**

## なぜ gpt-4 のクォータ消費がこんなに速いのですか？
- gpt-4 の消費速度は gpt-3.5-turbo の 20 から 40 倍です。90,000 token を購入したと仮定し、平均倍率として 30 倍を使用すると、90000 / 30 = 約 3000 文字となります。加えて、毎回履歴メッセージを添付する必要があるため、送信できるメッセージ数はさらに半減します。最極端な場合、1つのメッセージで 90,000 token を完全に消費する可能性があるため、慎重に使用してください。   

## Next Web を使用する際、token を節約するための小さなコツはありますか？
- 対話ボックス上部の設定ボタンをクリックし、その中の設定項目を見つけてください：  
  - 履歴メッセージ数の持ち運び：数が少ないほど token 消費が少なくなりますが、同時に gpt は以前の対話を忘れます  
  - 履歴要約：長期的なトピックを記録するために使用され、オフにすると token 消費を削減できます  
  - システムレベルプロンプトの注入：ChatGPT の回答品質を向上させるために使用され、オフにすると token 消費を削減できます  
- 左下角の設定ボタンをクリックし、自動タイトル生成をオフにすると、token 消費を削減できます  
- 対話中に対話ボックス上部のロボットアイコンをクリックすると、モデルを素早く切り替えることができます。まず 3.5 を優先的に使用して質問し、回答に満足できない場合は 4.0 に切り替えて再質問できます。  

## なぜバックエンドで key を作成しても使用クォータが表示されないのですか？
無制限クォータに設定すると、使用クォータは更新されません。無制限クォータを有限クォータに変更すれば表示されます  

## ユーザー規約  
支払いは本規約への同意とみなされます！そうでなければ支払わないでください！  
1. 本サービスは、いかなる形でもユーザーのチャット情報を永続的に保存することはありません；  
2. 本サービスは、ユーザーが本サービス上で送信するテキストコンテンツを知ることも知る手段もありません。ユーザーが本サービスの使用により引き起こした違法犯罪の結果については、使用者が責任を負い、本サービスはそれに起因する可能性のある関連調査に全面的に協力します；