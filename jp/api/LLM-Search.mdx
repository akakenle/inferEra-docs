---
title: "LLMモデルのインターネット検索"
description: " "
icon: 'globe'
---

## 1️⃣ リアルタイムインターネットサポート：LLMの時効制限を打ち破り、より正確で信頼性の高い出力を実現

OpenAIおよびGeminiシリーズの大規模モデルインターフェースに、最新のネットワーク情報を取得する機能を追加しました。これにより、以下のことが可能になります。
✅ **最新情報の取得**：今日のホットトピック、最新の研究、リアルタイムデータなど、即座に取得できます。
✅ **知識の盲点を解消**：大規模モデルのトレーニングデータの時間制限を突破し、トレーニング後の新しい情報を取得できます。
✅ **幻覚のリスクを低減**：リアルタイムのインターネット検索に基づいた事実に基づいた回答により、AIが誤った情報を返す可能性を大幅に低減します。
✅ **意思決定の質を向上**：最新の事実に基づいた分析と提案により、意思決定に自信を持てます。

**サポートされているモデル：**
現在、OpenAIおよびGeminiの大規模モデルシリーズをサポートしており、2つのアクセス方法があります。

**1. ネイティブ検索機能モデル：**
**Geminiシリーズ** (Google検索と連携)：
- gemini-2.0-pro-exp-02-05-search
- gemini-2.0-flash-exp-search
- gemini-2.0-flash-search

**OpenAIシリーズ** (Bing検索)：
- gpt-4o-search-preview
- gpt-4o-mini-search-preview

**2. パラメータサポート方式：**
`web_search_options={}`パラメータを追加するだけで、すべてのGemini、OpenAI大規模モデルでインターネット接続機能を有効にできます。Geminiシリーズの検索料金は3.5ドル/千回です。


### 使用方法

使用する前に、`pip install -U openai`を実行してOpenAIパッケージをアップグレードする必要があります。

**例：**
<CodeGroup>
```py Python
from openai import OpenAI

client = OpenAI(
    api_key="sk-***", # AiHubMixで生成したキーに置き換えてください
    base_url="https://aihubmix.com/v1"
)

chat_completion = client.chat.completions.create(
    model="gemini-2.0-flash-exp",
    # 🌐 検索を有効にする
    web_search_options={},
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "大規模モデルAPIプラットフォームAIhubmixに関する情報を検索し、簡単に紹介し、関連リンクを提供してください。"
                }
            ]
        }
    ]
)

print(chat_completion.choices[0].message.content)
```

```ts Typescript 
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: 'sk-***',
    baseURL: 'https://aihubmix.com/v1'
});

async function main() {
    const chatCompletion = await client.chat.completions.create({
        model: 'gemini-2.0-flash-exp',
        // 🌐 検索を有効にする
        web_search_options: {},
        messages: [
            {
                role: 'user',
                content: [
                    {
                        type: 'text',
                        text: '大規模モデルAPIプラットフォームAIhubmixに関する情報を検索し、簡単に紹介し、関連リンクを提供してください。'
                    }
                ]
            }
        ]
    });

    console.log(chatCompletion.choices[0].message.content);
}

main().catch(console.error);
```

```shell Curl
curl "https://aihubmix.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer xxx" \
    -d '{
    "model": "gemini-2.0-flash-exp",
    "web_search_options": {},
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Google Arts & Cultureウェブサイトのゴッホに関する情報を提供し、簡単に紹介し、関連リンクを提供してください。"
                }
            ]
        }
    ],
    "stream": false
}'
```
</CodeGroup>

## 2️⃣ スマートサーフィン：AIにインターネットを自由に駆け巡らせる

モデルIDの末尾に`:surfing`を追加するだけで、あらゆる大規模言語モデルに検索機能を持たせることができます。
- 接尾辞を追加するだけで、複雑な統合は不要です。
- この方法では、ユーザーのリクエストはデフォルトで**Tavily検索サービス**に転送され、LLMは返された検索結果を参照して回答を作成します。
- 検索料金は**1回あたり0.006ドル**です。
- 現在、「ログ詳細」には各検索の料金は表示されていません。料金は「クレジット変動」から直接差し引かれますが、後で表示される予定です。

<Tip>
モデルIDは[モデル一覧](https://aihubmix.com/models)からコピーできます。
</Tip>

**例：**
<CodeGroup>
```py Python
import requests
import json
import os

try:
    response = requests.post(
        url="https://aihubmix.com/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {os.environ.get('AIHUBMIX_API_KEY')}",
            "Content-Type": "application/json",
        },
        data=json.dumps({
            "model": "gpt-4o-mini:surfing", # モデルIDの末尾に:surfingを追加するだけで検索をサポート
            "messages": [
                {
                    "role": "user",
                    "content": "ChatGPTのメモリ機能に関する最新の事実を検索し、URLを付けて中国語で回答してください"
                }
            ]
        })
    )

    result = response.json()
    print("API応答：", json.dumps(result, ensure_ascii=False, indent=2))

except requests.exceptions.RequestException as e:
    print(f"リクエストエラー：{e}")
except json.JSONDecodeError as e:
    print(f"JSON解析エラー：{e}")
except Exception as e:
    print(f"その他のエラー：{e}")
```
</CodeGroup>

**API応答例：**
```json
{
  "id": "chatcmpl-BLMY8YIKvcjNpiFmyvIfEGQMvPAAh",
  "model": "gpt-4o-mini-2024-07-18",
  "object": "chat.completion",
  "created": 1744431268,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ChatGPTは最近、大幅なメモリ機能のアップグレードを受け、ユーザーの過去のすべての会話を参照して、よりパーソナライズされた応答を提供できるようになりました。ユーザーは、ChatGPTにこの情報を記憶させないように選択したり、メモリ機能を完全に無効にしたりできます。この更新の詳細については、次のURLで確認できます：[https://www.digitaltrends.com/computing/openai-chatgpt-memory-update/](https://www.digitaltrends.com/computing/openai-chatgpt-memory-update/)"
      },
      "finish_reason": "stop"
    }
  ],
  "system_fingerprint": "fp_b705f0c291",
  "usage": {
    "prompt_tokens": 584,
    "completion_tokens": 99,
    "total_tokens": 683,
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    },
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  }
}
```